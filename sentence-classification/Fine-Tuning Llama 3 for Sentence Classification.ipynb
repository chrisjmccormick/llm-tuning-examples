{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisjmccormick/llm-tuning-examples/blob/main/sentence-classification/Fine-Tuning%20Llama%203%20for%20Sentence%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkGx7neZfcvy"
      },
      "source": [
        "# ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej"
      },
      "source": [
        "# LLM Fine-Tuning for Sentence Classification\n",
        "\n",
        "By Chris McCormick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJR6t_gCQe_x"
      },
      "source": [
        "I've been curious to see just how well these enormous LLMs perform at traditional NLP tasks such as classifying text.\n",
        "\n",
        "In this Notebook, I've taken my original \"BERT for Sequence Classification\" example and swapped out the 110M parameter BERT for the _8 billion_ parameter Llama 3.1.\n",
        "\n",
        "It's not a simple drop-in replacement, though:\n",
        "\n",
        "1. There are some techniques we'll need to employ in order to be able to fine-tune an LLM on a free Colab GPU without running out of memory.\n",
        "2. To get decent performance, you'll want to add a prompt to your dataset.\n",
        "\n",
        "I'll take us through each of these changes in this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_LOg_K18Gr_"
      },
      "source": [
        "\n",
        "_Seeing \"Under the Hood\"_\n",
        "\n",
        "One of my favorite aspects of the original Notebook was that it It implements the training loop in PyTorch rather than relying on the HuggingFace Trainer class, which I feel tends to hide too much of what's going on, so I've kept that aspect of the Notebook and updated it.\n",
        "\n",
        "It still uses HuggingFace for the model implementation and weights, and for tokenizing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFA9Jk9Hpmu6"
      },
      "source": [
        "# How to Apply An LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07u9LjFwu7WI"
      },
      "source": [
        "There are some different approaches you can take to classifying text with a language model (i.e., a \"Decoder\" model) like GPT or Llama.\n",
        "\n",
        "In this Notebook, we'll be doing it by:\n",
        "\n",
        "1. Adding a \"few-shot\" prompt to our text,\n",
        "2. Choosing words to represent our class labels, and\n",
        "3. Classifying the input using Llama 3.1's existing language modeling head to predict the label word.\n",
        "\n",
        "To see why this is a good choice, let's start with a brief overview of how Decoder models work, and then look at the problem with the simpler \"just classify the last token in the sequence\" approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe8lnWoam5d1"
      },
      "source": [
        "**Decoder Models**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni0yQWA6ninz"
      },
      "source": [
        "A Decoder model like Llama 3 processes one token at a time, and caches what it learns about it.\n",
        "\n",
        "It starts with looking up the **embedding** for the current input token, and then sends this through 24 \"decoder\" layers.\n",
        "\n",
        "> _An \"embedding\" is just an array of values--4,096 of them in the case of Llama 3. The values are a bunch of (very precise) fractions, and taken together they store information relevant to our task._\n",
        "\n",
        "Each layer \"enriches\" the embedding with a little more of what we need--a layer gathers information by looking back over the input text, and incorporates that into the embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-I0F773nheS"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1QD1pTMeQ3F6XwK03lbeUk45CuUZv28Py' alt='Inner Workings of An LLM' width='900' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8VXFeJypPJi"
      },
      "source": [
        "As it moves through the layers, the embedding goes from representing the current **input word**, to becoming an embedding that resembles the **next token** to predict!\n",
        "\n",
        "> _Fun fact: When you multiply and sum up each of the values in two word embeddings, the result is a measure of the similarity between the words!_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Meo30o8G4F6p"
      },
      "source": [
        "**Adding a Classifier Head**\n",
        "\n",
        "To classify the text, you could add a simple linear classifier (it's just 4,096 weight values, same as the embedding) which you apply to the final output embedding in the sequence.\n",
        "\n",
        "> _This is what the \"LlamaForSequenceClassification\" class in HuggingFace does for you, but we'll be going a different direction._\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhZWtQCwX1v0"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1Myo8C4Xp_wdMd6o2N4Qd2G6dqUeG4_I4' alt='Adding linear classifier' width='800' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ6JrTz_XxbL"
      },
      "source": [
        "**Without Prompting...**\n",
        "\n",
        "We _could_ stop here, and fine-tune our model without touching our dataset--I've seen it done in other examples.\n",
        "\n",
        "Think about what's happening in that approach, though...\n",
        "\n",
        "The original model was trained to predict the next token, and doesn't know that we want it to evalute the input text for grammatical correctness.\n",
        "\n",
        "So, at least initially, that output embedding contains little-to-no information relevant to our intended task.\n",
        "\n",
        "We'll have to fine-tune on enough training data to change Llama 3 from a \"next token predictor\" into a \"grammar evaluator\".\n",
        "\n",
        "That's certainly possible, but we can get better results with much less data if we're willing to add a prompt to the input text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLgrfPfSuGfM"
      },
      "source": [
        "**Prompting**\n",
        "\n",
        "We know that LLM's are already quite good at all kinds of tasks if you just explain and/or demonstrate the desired behavior in the prompt (i.e., \"few shot prompting\").\n",
        "\n",
        "We can craft a prompt to add to our input which will encourage the model to predict a token representing the correct class label. This will be a much better starting point for fine-tuning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGXeiAtM3XmW"
      },
      "source": [
        "\n",
        "**Our Initial Prompt**\n",
        "\n",
        "We're going to use the Llama 3 \"base model\" (as opposed to the \"instruction-tuned\" version which has been trained to behave more like an assistant).\n",
        "\n",
        "With the base model, I learned that it's best to prompt it by simply creating a pattern, like:\n",
        "\n",
        "```\n",
        "Me and him went to the store. - unacceptable\n",
        "Him and I went to the store. - acceptable\n",
        "The gardener watered the flowers. - acceptable\n",
        "I didn't eat any ice cream and any cake. -\n",
        "```\n",
        "\n",
        "With such a clear pattern, even if the model predicts the wrong answer, it's still very likely to output an embedding that resembles either \"acceptable\" or \"unacceptable\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhKsiRoS0z6u"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1UwhdmEfZQBbwHUxNWLf-rSTl3rlgwOT6' alt='Adding a few-shot prompt' width='800' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl9LAgV40yuF"
      },
      "source": [
        "Distinguishing between those two word embeddings is an easy task for a linear classifier--it shouldn't take much training to start getting decent results.\n",
        "\n",
        "And now as we further fine-tune the LLM, we're not fighting against its original behavior of next token prediction!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L56j1_9rZ9Mw"
      },
      "source": [
        "**Leveraging the LM Head**\n",
        "\n",
        "Of course, LLMs _already include_ the ability to say whether \"acceptable\" or \"unacceptable\" is most likely!\n",
        "\n",
        "This is performed by the model's \"language modeling head\" (LM head) for performing next token prediction.\n",
        "\n",
        "> _\"Language modeling\" is literally \"modeling all of human language\" by understanding the propbabilties.  \n",
        "\n",
        "The LM head is actually just a huge table of word embeddings (one for every token in the vocabulary), just like the vocabulary of input embeddings used at the beginning of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6u4Nue9B4x4"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1a0tDHIU-nLgC9_lnDvsts0lvw2vIuXVY' alt='Predicting a class label with an LLM' width='800' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2C9m4_iB7uX"
      },
      "source": [
        "When generating text, to predict the next token we **multiply** the **output embedding** with every word embedding in the **LM head**, giving us a score for each and every token in the model's vocabulary.\n",
        "\n",
        "These scores represent how likely the model thinks each one could be the next word.\n",
        "\n",
        "For our purposes, we can just look specifically at the scores for the words we've chosen to represent our **class labels** and choose whichever one has the **higher confidence**.\n",
        "\n",
        "(And note how this conveniently avoids any problems around the model predicting a word that's not one of our class labels!)\n",
        "\n",
        "Because of how intelligent these models have become, this approach of prompting and classifying with the LM head gives very strong results on our task, _even without any fine-tuning_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_10GUSE1CxVo"
      },
      "source": [
        "_Side Note: Why not use the same word embeddings for the input and output?_\n",
        "\n",
        "It _is_ possible to tie the input and output vocabularies together during pre-training so that we only need one table (which, in the case of Llama 3, would save us _half-a-billion_ parameters üò≥). However, it's standard practice (so presumably works better?) to allow the model to learn separate tables for the input and output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYSfBIy_Hzen"
      },
      "source": [
        "**Crafting the Prompt**\n",
        "\n",
        "It makes sense that thinking about your wording and defining a task more clearly in the prompt will improve performance, particularly when your task is rather complicated.\n",
        "\n",
        "In the example in this notebook, though, the task is very straightforward, and I was surprised to see just how big of an impact prompt engineering made, despite all of the different prompts being equally clear (it seemed to me) about what the objective was.\n",
        "\n",
        "While some of the prompt selection is just trial and error, there _are_ some very important subtleties to the tokenization process to pay close attention to, and I'll cover those.\n",
        "\n",
        "Let's dive into the code!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://www.chrismccormick.ai/subscribe?utm_source=colab&utm_medium=banner&utm_campaign=newsletter&utm_content=post10\"><img src=\"https://lh3.googleusercontent.com/d/1JIQOdjp869nHAoob3Zh5PLBb3CpvgJOO\" width=\"400\"/></a>"
      ],
      "metadata": {
        "id": "OD_GgF_4R7H3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNSxGezHBoMc"
      },
      "source": [
        "# ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV"
      },
      "source": [
        "# S1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu59smNlD4_O"
      },
      "source": [
        "## 1.1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRDl857_gQIK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Track the total time to run all of the package installations.\n",
        "t0 = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ0twhgM3dXK"
      },
      "source": [
        "Many HuggingFace packages are installed by default in Colab, but here are some we'll need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfK8v2GgMzjc",
        "outputId": "9d747560-76e9-4a90-a5f9-9bd107d2cfa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.44.1\n"
          ]
        }
      ],
      "source": [
        "# Required for using 4-bit quantization (to make our model fit in the GPU\n",
        "# memory!) and the 8-bit optimizer (which helps with running training on the\n",
        "# limited memory of the free T4 GPU).\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prN0gxG84HF6"
      },
      "source": [
        "The HuggingFace `peft` library implements LoRA for us, an important piece for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgu0lXO_Hk-l",
        "outputId": "2567a82d-48f4-4b7e-ea7d-89edf6fdace3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting peft\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "Successfully installed peft-0.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lHTFKepLJLH"
      },
      "source": [
        "## 1.2. Llama Access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx1d32MSL0eP"
      },
      "source": [
        "One small annoyance here is that, in order to use recent LLMs like Llama 3.1, you'll need to accept a license agreement for the model, and then perform a login step here so that HuggingFace knows you've done that.\n",
        "\n",
        "1. Create a huggingface account (it's free).\n",
        "2. Go to the model page and fill out the form:\n",
        "    * https://huggingface.co/meta-llama/Meta-Llama-3-8B\n",
        "    * I didn't get access immediately, but I had access when I came back to check two hours later. You can check status [here](https://huggingface.co/settings/gated-repos), and I also received an email notification.\n",
        "3. Create an \"access token\" in your account settings:\n",
        "    * Go to your [token settings](https://huggingface.co/settings/tokens) and click \"New Token\":\n",
        "\n",
        "<center><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new-token.png\" width=\"500px\" alt_text=\"Create a new token\"/>\n",
        "\n",
        "(Image is from the HuggingFace docs, <a href=\"(https://huggingface.co/docs/hub/en/security-tokens\">here</a>)</center>\n",
        "\n",
        "* You can name it **hf_hub_all_notebooks**, and the **read** Role is sufficient.\n",
        "\n",
        "4. Add it to the **Secrets** panel in Colab (the key symbol in the sidebar to the left).\n",
        "    * Click the checkbox to give your notebook access to the key.\n",
        "5. Finally, run the code below to log in to HuggingFace!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yxr7YwO_o2k",
        "outputId": "c5075f29-779c-40ca-bfa7-2fef208e5772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# I used the \"secrets\" panel in Colab and defined the variable\n",
        "# \"hf_hub_all_notebooks\" and set it to my personal huggingface key.\n",
        "# You could just paste in your key directly here if this is a private copy.\n",
        "hf_api_key = userdata.get('hf_hub_all_notebooks')\n",
        "\n",
        "!huggingface-cli login --token $hf_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY4ViKsVb0tQ"
      },
      "source": [
        "## 1.3. GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QG1i8rUsIWt"
      },
      "source": [
        "The settings for this Notebook depend a bit on your choice of GPU. This example runs quite comfortably on the A100 or L4, but the free T4 is more constraining...\n",
        "\n",
        "If you're running on the T4, we'll use the \"8-bit optimizer\", and possibly gradient accumulation depending on your maximum sequence length.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We'll use a function from PyTorch to get some details on the GPU you're using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z1_Qnn_s8un",
        "outputId": "43ae3e76-0f71-41d0-f30c-21a686c52e2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4.\n",
            "GPU Memory = 14.748 GB\n",
            "Shorthand Name: Tesla T4  --> T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# There's a nice function now for retrieving the GPU info...\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "\n",
        "print(f\"GPU = {gpu_stats.name}.\")\n",
        "\n",
        "# A key attribute is the amount of memory, in GB, the GPU has, so let's\n",
        "# calculate that by dividing the total bytes by 2^30.\n",
        "gpu_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "print(f\"GPU Memory = {gpu_memory} GB\")\n",
        "\n",
        "# Finally, we'll store a shorthand version of the name and use this as a switch\n",
        "# for other settings (e.g., if gpu = \"T4\", then ...)\n",
        "if \"T4\" in gpu_stats.name:\n",
        "    gpu = \"T4\"\n",
        "\n",
        "elif \"L4\" in gpu_stats.name:\n",
        "    gpu = \"L4\"\n",
        "\n",
        "elif \"A100\" in gpu_stats.name:\n",
        "    gpu = \"A100\"\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported GPU: {gpu_stats.name}\")\n",
        "\n",
        "print(\"Shorthand Name:\", gpu_stats.name, ' -->', gpu )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF_p2XF-LsDU"
      },
      "source": [
        "## 1.4. Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJUzYeGzvEaR"
      },
      "source": [
        "`format_time`\n",
        "\n",
        "For printing out how long certain steps took."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtoglSugH7fq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFhjqFsekQvl",
        "outputId": "d4b1ec70-f199-40da-b2b2-e9dcd6c1844c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All of that package installation stuff took: 0:00:17\n"
          ]
        }
      ],
      "source": [
        "print(\"All of that package installation stuff took:\", format_time(time.time() - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0gamLDFx6Ca"
      },
      "source": [
        "`format_size`\n",
        "\n",
        "This helper function prints out big numbers nicely using base 2 magnitudes (i.e., K = 2^10, M = 2^20, B = 2^30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01Qv4iUQx6Cb"
      },
      "outputs": [],
      "source": [
        "def format_size(num):\n",
        "    \"\"\"\n",
        "    This function iterates through a list of suffixes ('K', 'M', 'B') and\n",
        "    divides the input number by 1024 until the absolute value of the number is\n",
        "    less than 1024. Then, it formats the number with the appropriate suffix and\n",
        "    returns the result. If the number is larger than \"B\", it uses 'T'.\n",
        "    \"\"\"\n",
        "    suffixes = ['', 'K', 'M', 'B']\n",
        "    base = 1024\n",
        "\n",
        "    for suffix in suffixes:\n",
        "        if abs(num) < base:\n",
        "            if num % 1 != 0:\n",
        "                return f\"{num:.2f}{suffix}\"\n",
        "\n",
        "            else:\n",
        "                return f\"{num:.0f}{suffix}\"\n",
        "\n",
        "        num /= base\n",
        "\n",
        "    # Use \"T\" for anything larger.\n",
        "    if num % 1 != 0:\n",
        "        return f\"{num:.2f}T\"\n",
        "\n",
        "    else:\n",
        "        return f\"{num:.0f}T\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyN64KieaI-d"
      },
      "source": [
        "`format_lr_as_multiple`\n",
        "\n",
        "Learning rates are usually something times 10^-4 or 10^-5. I've started to display them as multiples of the smallest typical value--1e-5.\n",
        "\n",
        "I think that makes it a lot easier to understand the relative size of the different lr values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79W2WsV2eeb7"
      },
      "outputs": [],
      "source": [
        "def format_lr_as_multiple(lr):\n",
        "    # Convert the learning rate into a multiple of 1e-5.\n",
        "    multiple = lr / 1e-5\n",
        "\n",
        "    # Return the formatted string.\n",
        "    return \"{:.1f} x 1e-5\".format(multiple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bRXf3QkajtU"
      },
      "source": [
        "`gpu_mem_used`\n",
        "\n",
        "This function uses the \"NVIDIA System Management Interface\" `nvidia-smi` command line tool to retrieve the current memory usage.\n",
        "\n",
        "There's a function in PyTorch, `torch.cuda.memory_allocated()`, but it seems to severely under-report. ü§∑‚Äç‚ôÇÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux196nNpIqbW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def gpu_mem_used():\n",
        "    \"\"\"\n",
        "    Returns the current GPU memory usage as a string, e.g., \"5.02 GB\"\n",
        "    \"\"\"\n",
        "\n",
        "    # This approach doesn't work, because PyTorch only tracks its own memory\n",
        "    # usage, not the total memory consumption of the GPU.\n",
        "    #gpu_bytes_used = torch.cuda.memory_allocated()\n",
        "\n",
        "    # Run the nvidia-smi command line tool to get memory used in megabytes.\n",
        "    buf = os.popen('nvidia-smi --query-gpu=memory.used, --format=csv,noheader,nounits')\n",
        "\n",
        "    # It returns an unformated integer number of \"MiB\" (2^20 bytes).\n",
        "    gpu_mb_used = float(buf.read())\n",
        "\n",
        "    # Divide that by 1024 to get GB.\n",
        "    mem_used = gpu_mb_used / float(1024)\n",
        "\n",
        "    return (\"{0:.2f} GB\".format(mem_used))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4qRq_h3MHSo",
        "outputId": "346d8f43-dfb8-4abf-a331-6a80a12d0aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory used: 0.00 GB\n"
          ]
        }
      ],
      "source": [
        "print(\"GPU memory used: {:}\".format(gpu_mem_used()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb2DR8lwBbTK"
      },
      "source": [
        "# ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc"
      },
      "source": [
        "# S2. Loading CoLA Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ZKxKc04Btk"
      },
      "source": [
        "We're comparing LLM performance to my original BERT Notebook, which used [The Corpus of Linguistic Acceptability (CoLA)](https://nyu-mll.github.io/CoLA/) dataset for single sentence classification. It's a set of sentences labeled as grammatically correct or incorrect. It was first published in May of 2018, and is one of the tests included in the \"GLUE\" Benchmark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JrUHXms16cn"
      },
      "source": [
        "## 2.1. Download & Extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZNVW6xd0T0X"
      },
      "source": [
        "We'll use the `wget` package to download the dataset to the Colab instance's file system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m6AnuFv0QXQ",
        "outputId": "82342693-c5cd-412e-a16a-72c891338e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=141f23cd1eb8ec1c7f61d7e0a90630b1c3d65485191ceb05896a23c0a1f6006c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "# (Not installed by default)\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08pO03Ff1BjI"
      },
      "source": [
        "The dataset is hosted on GitHub in this repo: https://nyu-mll.github.io/CoLA/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMtmPMkBzrvs",
        "outputId": "468d6c57-8ca2-424e-a4f6-2b1dfc414572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ]
        }
      ],
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mKctx-ll2FB"
      },
      "source": [
        "Unzip the dataset to the file system. You can browse the file system of the Colab instance in the sidebar on the left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yv-tNv20dnH",
        "outputId": "a6cbde20-4675-4b6c-a9e5-20ce73cf1ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  cola_public_1.1.zip\n",
            "   creating: cola_public/\n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/\n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
          ]
        }
      ],
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQUy9Tat2EF_"
      },
      "source": [
        "## 2.2. Parse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeyVCXT31EZQ"
      },
      "source": [
        "We can see from the file names that both `tokenized` and `raw` versions of the data are available.\n",
        "\n",
        "We can't use the pre-tokenized version because, in order to apply an LLM, we *must* use the tokenizer provided by the model--it has a specific vocabulary of tokens that it can accept."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYWzeGSY2xh3"
      },
      "source": [
        "We'll use pandas to parse the \"in-domain\" data (the training set) and look at a few of its properties and data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "_UkeC7SG2krJ",
        "outputId": "344be884-4c19-404b-a56e-053978a40b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"sentence_source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"l-93\",\n          \"ks08\",\n          \"c_13\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_notes\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"*\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The statue stood on the pedestal.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-cf9b441d-a14a-417a-971b-1eb7096909ce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3151</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sharon shivered.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4187</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Some of the record contains evidence of wrongd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7517</th>\n",
              "      <td>sks13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Mary wrote a letter to him last year.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7220</th>\n",
              "      <td>sks13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>It is arrive tomorrow that Mary will.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8371</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The wizard turned the beetle into beer with a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8385</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I looked the number up.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3683</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>John suddenly put the customers off.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The kettle bubbled water.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2191</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The statue stood on the pedestal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5984</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Alex was eating the popsicle.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf9b441d-a14a-417a-971b-1eb7096909ce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cf9b441d-a14a-417a-971b-1eb7096909ce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cf9b441d-a14a-417a-971b-1eb7096909ce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-25826786-44c2-42b3-a837-ccfe78ffffef\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-25826786-44c2-42b3-a837-ccfe78ffffef')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-25826786-44c2-42b3-a837-ccfe78ffffef button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     sentence_source  label label_notes  \\\n",
              "3151            l-93      1         NaN   \n",
              "4187            ks08      1         NaN   \n",
              "7517           sks13      1         NaN   \n",
              "7220           sks13      0           *   \n",
              "8371            ad03      1         NaN   \n",
              "8385            ad03      1         NaN   \n",
              "3683            ks08      1         NaN   \n",
              "599             bc01      1         NaN   \n",
              "2191            l-93      1         NaN   \n",
              "5984            c_13      1         NaN   \n",
              "\n",
              "                                               sentence  \n",
              "3151                                   Sharon shivered.  \n",
              "4187  Some of the record contains evidence of wrongd...  \n",
              "7517              Mary wrote a letter to him last year.  \n",
              "7220              It is arrive tomorrow that Mary will.  \n",
              "8371  The wizard turned the beetle into beer with a ...  \n",
              "8385                            I looked the number up.  \n",
              "3683               John suddenly put the customers off.  \n",
              "599                           The kettle bubbled water.  \n",
              "2191                  The statue stood on the pedestal.  \n",
              "5984                      Alex was eating the popsicle.  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\n",
        "    \"./cola_public/raw/in_domain_train.tsv\",\n",
        "    delimiter = '\\t',\n",
        "    header = None,\n",
        "    names = ['sentence_source', 'label', 'label_notes', 'sentence']\n",
        ")\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfWzpPi92UAH"
      },
      "source": [
        "The two properties we actually care about are the the `sentence` and its `label`, which is referred to as the \"acceptibility judgment\" (0=unacceptable, 1=acceptable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_LpQfzCn9_o"
      },
      "source": [
        "Here are five sentences which are labeled as not grammatically acceptible. Note how much more difficult this task is than something like sentiment analysis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "blqIvQaQncdJ",
        "outputId": "7bd46b10-4e48-4433-899a-db5ddb42a6c8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I wonder you ate how much.\",\n          \"It's probable in general that he understands what's going on.\",\n          \"Terry touched at the cat.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3ce0a0b6-7c3f-4cca-8176-9ba7bbfac52c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7894</th>\n",
              "      <td>No reading Shakespeare satisfied me</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>I wonder you ate how much.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2154</th>\n",
              "      <td>Terry touched at the cat.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2790</th>\n",
              "      <td>Steve pelted acorns to Anna.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>It's probable in general that he understands w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ce0a0b6-7c3f-4cca-8176-9ba7bbfac52c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3ce0a0b6-7c3f-4cca-8176-9ba7bbfac52c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3ce0a0b6-7c3f-4cca-8176-9ba7bbfac52c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1f7e9479-b808-451c-867c-b35db0aa5199\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f7e9479-b808-451c-867c-b35db0aa5199')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1f7e9479-b808-451c-867c-b35db0aa5199 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "7894                No reading Shakespeare satisfied me      0\n",
              "244                          I wonder you ate how much.      0\n",
              "2154                          Terry touched at the cat.      0\n",
              "2790                       Steve pelted acorns to Anna.      0\n",
              "430   It's probable in general that he understands w...      0"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2YfSEZVUJ9d"
      },
      "source": [
        "Also, note that this dataset is highly imbalanced--let's print out the number of positive vs. negative samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQWz0g4uT-Sm",
        "outputId": "4f2247e1-69fc-423f-85de-b7ed8f11c5d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of positive samples: 6,023 of 8,551. 70.4%\n"
          ]
        }
      ],
      "source": [
        "# Since the positive samples have the label value \"1\", we can sum them to count\n",
        "# them.\n",
        "num_positive_samples = df.label.sum()\n",
        "\n",
        "num_samples = len(df.label)\n",
        "\n",
        "prcnt_positive = float(num_positive_samples / num_samples) * 100.0\n",
        "\n",
        "print(\n",
        "    \"Number of positive samples: {:,} of {:,}. {:.3}%\".format( \\\n",
        "    num_positive_samples, num_samples, prcnt_positive)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SMZ5T5Imhlx"
      },
      "source": [
        "\n",
        "\n",
        "Let's extract the sentences and labels of our training set as numpy ndarrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuE5BqICAne2"
      },
      "outputs": [],
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnDLceDhNzjg"
      },
      "source": [
        "# S3. Prompting & Label Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4eK67dYtPdR"
      },
      "source": [
        "As covered in the intro, the prompt and label words are key to our success.\n",
        "\n",
        "In this section, we'll look at how the tokenizer handles our prompt and label word choices.\n",
        "\n",
        "Then we'll add the prompt and label words to our data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "## 3.1. Tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWOPOyWghJp2"
      },
      "source": [
        "\n",
        "To feed our text to an LLM, it must be split into tokens, and then these tokens must be mapped to their index in the model's vocabulary.\n",
        "\n",
        "The tokenization must be performed by the tokenizer included with Llama--the below cell will download this for us.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "25e15a54c17f453480c5706f616d2de2",
            "a2fa0ba2d7924e81b1d01f2f60eb4c8e",
            "f08af515bc0b49f4b118f0110a38558b",
            "cf6186da4e02447db484641ecb81d205",
            "aa5d9e68044449869f81755e773db193",
            "abac6daa19454bf3ac10a3bb0f6ca80d",
            "8eaf622ece844de2a00ce689941750f0",
            "9190880502544e2fa42953da4951602e",
            "ad4e8c85524d49b28145f577c47fd224",
            "a661efbb8fd14bc3b5bf04269f7fbe8a",
            "ec402c957ee5443b9e4ecec3e65b06be",
            "b53df4bf66c042f68f6c8d8fcd847603",
            "fa893bdb0afe44899a1c9f17d21ce8b2",
            "ecbbe66244e3436db171eef9f8a1ab9a",
            "8ac10cb452f24e69be750244c9c418fd",
            "edf9aa84db0f4f31b26681738929f928",
            "59a75224171e4b40bd031d536165a979",
            "f2d8b2141fee43548ebec527896cfcc9",
            "92f495a5e2d84cc4853e406e47372e5f",
            "5720212272c2430f8de878f92a1d7767",
            "f76ce0b8f96043358ce49926638d82d8",
            "3507f0de88ab4580a2f64426d2ad2dbe",
            "886a4cb2659d4dbab3a92e4bd5d0d39f",
            "1b99c7bea81d46c489d3f9f285301a1b",
            "7ba2fb21ed06496f85692202845971ab",
            "cffc2fcc78fe455aa742d58f958e360d",
            "f17dacb729f9425ab63be302d0a1e2f1",
            "780353d8b2e04a718b679fe7a76400ab",
            "c9bbc091ab2b4b65acf72f022e10f048",
            "fe856fecb6b04b9cb2b31ca28560eee8",
            "026a1b99beb84eb987c20ab310725fb8",
            "e24939e3a5ce42a6831ee3a3366874eb",
            "dc81102677e84a4f86bae105104e3f7f"
          ]
        },
        "id": "K5N2rh6u6umq",
        "outputId": "8c97934a-1652-499a-f88a-30cd42078f73"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25e15a54c17f453480c5706f616d2de2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b53df4bf66c042f68f6c8d8fcd847603",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "886a4cb2659d4dbab3a92e4bd5d0d39f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import LlamaTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import LlamaTokenizerFast\n",
        "\n",
        "# Load the tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFzmtleW6KmJ"
      },
      "source": [
        "Let's apply the tokenizer to one sentence just to see the output.\n",
        "\n",
        "IMPORTANT: The Llama tokenizer distinguishes words and subwords by a leading space. (For Mistral, the token string contains a space. For Llama, it's 'ƒ†'. üôÑ In both cases, though, your input just needs a leading ' ')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLIbudgfh6F0",
        "outputId": "834501ed-be6f-4576-fb07-f5c50418aff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Original:   Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['ƒ†Our', 'ƒ†friends', 'ƒ†won', \"'t\", 'ƒ†buy', 'ƒ†this', 'ƒ†analysis', ',', 'ƒ†let', 'ƒ†alone', 'ƒ†the', 'ƒ†next', 'ƒ†one', 'ƒ†we', 'ƒ†propose', '.']\n",
            "Token IDs:  [5751, 4885, 2834, 956, 3780, 420, 6492, 11, 1095, 7636, 279, 1828, 832, 584, 30714, 13]\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: Add the leading space to the sentence!\n",
        "example_sen = ' ' + sentences[0]\n",
        "\n",
        "# Print the original sentence.\n",
        "print(' Original: ', example_sen)\n",
        "\n",
        "tokens = tokenizer.tokenize(example_sen)\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokens)\n",
        "\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMzafGqNwkPv"
      },
      "source": [
        "* Here are some interesting properties to check out of any tokenizer that you work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIRx308jwkYT",
        "outputId": "382c229b-6dea-4bf1-d938-ea92f07c40be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  name_or_path:  meta-llama/Meta-Llama-3-8B\n",
            "                    vocab_size:  128000\n",
            "              model_max_length:  1000000000000000019884624838656\n",
            "                       is_fast:  True\n",
            "                  padding_side:  right\n",
            "               truncation_side:  right\n",
            "  clean_up_tokenization_spaces:  True\n",
            "                     bos_token:  <|begin_of_text|>\n",
            "                     eos_token:  <|end_of_text|>\n",
            "                     unk_token:  None\n",
            "                     sep_token:  None\n",
            "                     pad_token:  None\n",
            "                    mask_token:  None\n"
          ]
        }
      ],
      "source": [
        "# Assume tokenizer is your LlamaTokenizer instance\n",
        "tokenizer_properties = {\n",
        "    'name_or_path': tokenizer.name_or_path,\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'model_max_length': tokenizer.model_max_length,\n",
        "    'is_fast': tokenizer.is_fast,\n",
        "    'padding_side': tokenizer.padding_side,\n",
        "    'truncation_side': tokenizer.truncation_side,\n",
        "    'clean_up_tokenization_spaces': tokenizer.clean_up_tokenization_spaces,\n",
        "    'bos_token': tokenizer.bos_token,\n",
        "    'eos_token': tokenizer.eos_token,\n",
        "    'unk_token': tokenizer.unk_token,\n",
        "    'sep_token': tokenizer.sep_token,\n",
        "    'pad_token': tokenizer.pad_token,\n",
        "    'mask_token': tokenizer.mask_token,\n",
        "}\n",
        "\n",
        "# Print each property in a formatted way\n",
        "for key, value in tokenizer_properties.items():\n",
        "    print(f\"{key.rjust(30)}:  {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACPTT8jXsVbE"
      },
      "source": [
        "**Tokenizer Settings**\n",
        "\n",
        "Padding our samples with a \"pad token\" is important for batching our inputs to the model (all the samples in a batch need to be the same length, so we have to pad them).\n",
        "\n",
        "We provide the model with an attention mask to ensure that it doesn't attend to the pad tokens (though, with an LLM, the real tokens can't look ahead to the padding anyway!)\n",
        "\n",
        "Since the pad tokens aren't incorporated into the interpretation of the text, it doesn't actually matter which token we use, and Llama doesn't even have one defined. We just need to pick something, so we'll use the eos_token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xYelx8wQGDb"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRp4O7D295d_"
      },
      "source": [
        "## 3.2. Choosing Label Words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JtNqua1Z3KO"
      },
      "source": [
        "Some thoughts:\n",
        "\n",
        "* Choose words that make sense for the task to leverage the LLM's understanding.\n",
        "* For CoLA, they use the language gramattically \"acceptable\" or \"unacceptable\" rather than \"correct\" or \"incorrect\".\n",
        "* This notebook won't support multi-token label words--you could probably do it if you think through the changes carefully.\n",
        "* Again, to treat the labels as whole words, you'll need to include the leading space.\n",
        "\n",
        "Let's see how the tokenizer handles our labels!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh7RKwBNaQgw",
        "outputId": "b71a0d5a-955d-45fe-89de-188b20acb9bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token representations of our label words:\n",
            "    ' acceptable' =  ['ƒ†acceptable']\n",
            "  ' unacceptable' =  ['ƒ†unacceptable']\n"
          ]
        }
      ],
      "source": [
        "print(\"Token representations of our label words:\")\n",
        "\n",
        "print(\"    ' acceptable' = \", tokenizer.tokenize(' acceptable'))\n",
        "print(\"  ' unacceptable' = \", tokenizer.tokenize(' unacceptable'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhrda90J4eY5"
      },
      "source": [
        "Out of curiosity, here are \"correct\" and \"incorrect\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mSPlaggULF8",
        "outputId": "1dfcc954-cc16-424f-e89b-841bc6832a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token representations of our label words:\n",
            "    ' correct' =  ['ƒ†correct']\n",
            "  ' incorrect' =  ['ƒ†incorrect']\n"
          ]
        }
      ],
      "source": [
        "print(\"Token representations of our label words:\")\n",
        "\n",
        "print(\"    ' correct' = \", tokenizer.tokenize(' correct'))\n",
        "print(\"  ' incorrect' = \", tokenizer.tokenize(' incorrect'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib6MXuUy4pDI"
      },
      "source": [
        "There's a very useful tool here for quickly viewing the tokens in a nicely formatted way:\n",
        "\n",
        "https://huggingface.co/spaces/Xenova/the-tokenizer-playground\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2PvPR7a462c"
      },
      "source": [
        "**Subwords vs. Words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syeT2eXZVekX"
      },
      "source": [
        "* SentencePiece tokenizers can reconstruct text fully.\n",
        "* Full words are distinguished from sub words using a leading space.\n",
        "* Llama 3 has a ~4x larger vocabulary than the previous generation (Llama 2 and Mistral 7b).\n",
        "\n",
        "* If you ommit the leading space they will be treated as subtokens, and may not actually have the meaning you intended!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4cn91eAUlnW",
        "outputId": "6c0d7707-7ca0-47fa-fc06-f510ac844f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token representations if we forget the leading space:\n",
            "    'acceptable' =  ['acceptable']\n",
            "  'unacceptable' =  ['un', 'acceptable']\n"
          ]
        }
      ],
      "source": [
        "print(\"Token representations if we forget the leading space:\")\n",
        "\n",
        "print(\"    'acceptable' = \", tokenizer.tokenize(\"acceptable\"))\n",
        "print(\"  'unacceptable' = \", tokenizer.tokenize(\"unacceptable\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKRi6VrYmpmi"
      },
      "source": [
        "We're using acceptable and unacceptable.\n",
        "\n",
        "The labels now have three representations:\n",
        "\n",
        "1. The class label, 0 or 1\n",
        "2. The token string, \" unacceptable\" or \" acceptable\"\n",
        "3. The token id, 44085, 22281\n",
        "\n",
        "At various points we'll need to map between them, so I've created some dictionaries for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHKEFxjYVZbB",
        "outputId": "533d5154-9952-42c5-9c30-5882fec36104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ƒ†acceptable\n",
            "22281\n",
            "\n",
            "ƒ†unacceptable\n",
            "44085\n",
            "\n"
          ]
        }
      ],
      "source": [
        "label_val_to_word = {}\n",
        "label_val_to_token_id = {}\n",
        "\n",
        "pos_token_id = -1\n",
        "neg_token_id = -1\n",
        "\n",
        "# Select our word for the positive label (1)\n",
        "label_val_to_word[1] = ' acceptable'\n",
        "\n",
        "# Show it as a token.\n",
        "pos_token = tokenizer.tokenize(label_val_to_word[1])[0]\n",
        "print(pos_token)\n",
        "\n",
        "# Lookup and store its token id.\n",
        "pos_token_id = tokenizer.convert_tokens_to_ids(pos_token)\n",
        "label_val_to_token_id[1] = pos_token_id\n",
        "print(str(pos_token_id) + '\\n')\n",
        "\n",
        "# Select our word for the negative label (0)\n",
        "label_val_to_word[0] = ' unacceptable'\n",
        "\n",
        "# Show it as a token.\n",
        "neg_token = tokenizer.tokenize(label_val_to_word[0])[0]\n",
        "print(neg_token)\n",
        "\n",
        "# Look up and store its token id.\n",
        "neg_token_id = tokenizer.convert_tokens_to_ids(neg_token)\n",
        "label_val_to_token_id[0] = neg_token_id\n",
        "print(str(neg_token_id) + '\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2nojm2-BOtn"
      },
      "source": [
        "## 3.3. Crafting Our Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ7feQVuxtnO"
      },
      "source": [
        "I'll share some prompts I tried and the scores they got in order to illustrate a few points.\n",
        "\n",
        "Note that, because the CoLA dataset is imbalanced (with more 'acceptable' sentences than 'unacceptable'), the official metric is the Matthews Correlation Coefficient (MCC), which is designed to account for this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dKK3rnpx0Wn"
      },
      "source": [
        "**Mistral vs. Llama 3**\n",
        "\n",
        "Here's the first prompt variation that I tried.\n",
        "\n",
        "```python\n",
        "\"\"\" Classify this sentence as grammatically acceptable or unacceptable: Him and me are going to the store.\n",
        "A: unacceptable\n",
        "\n",
        "Classify this sentence as grammatically acceptable or unacceptable: Him and I are going to the store.\n",
        "A: acceptable\n",
        "\n",
        "Classify this sentence as grammatically acceptable or unacceptable: {sentence}\n",
        "A:{label_word}\"\"\"\n",
        "```\n",
        "\n",
        "1. This scored an MCC value of `0.522` using Mistral 7B.\n",
        "\n",
        "2. Simply switching to **Llama 3.1** raised this by **0.023** to `0.545`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUmMBGrD9bXJ"
      },
      "source": [
        "**Leading Spaces and Label Quotes**\n",
        "\n",
        "I made a few more changes, explained below, and arrived at the following:\n",
        "\n",
        "```python\n",
        "\"\"\" Classify the grammar of this sentence as ' acceptable' or ' unacceptable': Him and me are going to the store.\n",
        " A: unacceptable\n",
        "\n",
        " Classify the grammar of this sentence as ' acceptable' or ' unacceptable': Him and I are going to the store.\n",
        " A: acceptable\n",
        "\n",
        " Classify the grammar of this sentence as ' acceptable' or ' unacceptable': {sentence}\n",
        " A:{label_word}\"\"\"\n",
        " ```\n",
        "\n",
        "1. The previous prompt is missing the **leading spaces** on each new line, and fixing that increased the score by **0.011** up to `0.556`.\n",
        "\n",
        "2. Adding **single quotes** around the the label words had the _biggest impact_ of anything I tried! It increased the score by **0.076** up to `0.632`.\n",
        "\n",
        "3. I was curious if adding a leading space to the label words _inside the quotes_ would help--it did make a small difference of 0.006, bringing the score to 0.638."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et-V-wjL0F_W"
      },
      "source": [
        "**Instructions vs. Patterns**\n",
        "\n",
        "I learned that when using the base model (`meta-llama/Meta-Llama-3-8B`), as opposed to the instruction-tuned version (`meta-llama/Llama-3.1-8B-Instruct`), it's best to create a simple pattern rather than telling it what to do.\n",
        "\n",
        "(It did still help a little, though, to provide some context as well) Here's what I landed on for the prompt in this notebook:\n",
        "\n",
        "```python\n",
        "\"\"\" Examples of sentences that are grammatically ' acceptable' or ' unacceptable':\n",
        " Him and me are going to the store. - unacceptable\n",
        " Him and I are going to the store. - acceptable\n",
        " {sentence} -{label_word}\"\"\"\n",
        "```\n",
        "\n",
        "This increased the score by **0.009** to `0.647`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp3u4UvaCx7Q"
      },
      "source": [
        "## 3.4. Add Prompts & Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syzFDAX3C4kh"
      },
      "source": [
        "We can define our prompt as a template (as a formatting string), and then loop over the samples to apply the template.\n",
        "\n",
        "Note that we're adding labels to the entire training set, and doing it prior to splitting off a validation set...\n",
        "\n",
        "But that's ok! When predicting the next token, the LLM can't attend to future tokens, so it can't \"see\" the label. And we want the label there anyway so that we can measure the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zuInbPr80cz"
      },
      "outputs": [],
      "source": [
        "prompt_template = \\\n",
        "\"\"\" Examples of sentences that are grammatically ' acceptable' or ' unacceptable':\n",
        " Him and me are going to the store. - unacceptable\n",
        " Him and I are going to the store. - acceptable\n",
        " {sentence} -{label_word}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tHXJlZO3P0d"
      },
      "outputs": [],
      "source": [
        "labeled_sentences = []\n",
        "labels_as_ids = []\n",
        "\n",
        "# For each sentence in the dataset...\n",
        "for i in range(len(sentences)):\n",
        "\n",
        "    sentence = sentences[i]\n",
        "    label_val = labels[i]\n",
        "\n",
        "    # Map the numerical label (0, 1) to the word we chose.\n",
        "    label_word = label_val_to_word[label_val]\n",
        "\n",
        "    # Look up the token id for the label.\n",
        "    label_id = label_val_to_token_id[label_val]\n",
        "\n",
        "    # Insert the sample and its label into the template.\n",
        "    labeled_sentence = prompt_template.format(\n",
        "        sentence = sentence,\n",
        "        label_word = label_word\n",
        "    )\n",
        "\n",
        "    # Add to our new lists.\n",
        "    labeled_sentences.append(labeled_sentence)\n",
        "    labels_as_ids.append(label_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc7w9Wra76YE"
      },
      "source": [
        "Let's check out a couple formatted examples.\n",
        "\n",
        "It can be difficult to recognize in the output window, but each line begins with a leading space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnYXhEhv76YE",
        "outputId": "a1aeee77-6000-4653-a6ab-1b30ea739154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\" Examples of sentences that are grammatically ' acceptable' or ' unacceptable':\n",
            " Him and me are going to the store. - unacceptable\n",
            " Him and I are going to the store. - acceptable\n",
            " Our friends won't buy this analysis, let alone the next one we propose. - acceptable\"\n"
          ]
        }
      ],
      "source": [
        "print(\"\\\"{:}\\\"\".format(labeled_sentences[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFh-Bind76YE",
        "outputId": "2b914d95-b7a1-4d62-ce24-983e25e09b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\" Examples of sentences that are grammatically ' acceptable' or ' unacceptable':\n",
            " Him and me are going to the store. - unacceptable\n",
            " Him and I are going to the store. - acceptable\n",
            " One more pseudo generalization and I'm giving up. - acceptable\"\n"
          ]
        }
      ],
      "source": [
        "print(\"\\\"{:}\\\"\".format(labeled_sentences[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2IfapojlPuc"
      },
      "source": [
        "**Subsample Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fkq1pv2jlT-6"
      },
      "outputs": [],
      "source": [
        "# If you want to quickly test out your code, you can slice this down to just a\n",
        "# small subset.\n",
        "\n",
        "#labeled_sentences = labeled_sentences[:500]\n",
        "#labels_as_ids = labels_as_ids[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYVRAtLnIZYO"
      },
      "source": [
        "# S4. Format Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KZpu4Z3AnYw"
      },
      "source": [
        "## 4.1. Tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoN6RMqhgQpL"
      },
      "source": [
        "Tokenize and encode all of the sentences.\n",
        "\n",
        "**Padding for Batching**\n",
        "\n",
        "* Training on batches of, e.g., 16 samples at once is an important part of achieving good accuracy. The weight updates are averaged over the batch, leading to smoother training steps.\n",
        "* It also makes better use of the GPU's parallelism and speeds things up substantially.\n",
        "* It introduces a problem, though... The batch is presented as a single tensor, so the dimensions of all 16 samples need to be the same.\n",
        "* We address this by adding garbage to the end, and specifying an input attention mask which zeros out the influence of those tokens on the weight updates.\n",
        "\n",
        "* Our label words will be at a different spot for each sample, depending on its length. We can look at the attention mask to easily locate them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "104xZTED9y4x"
      },
      "source": [
        "\n",
        "Some of the parameters I'm setting here correspond to the defaults, but I like being explicit.\n",
        "\n",
        "> A little side rant: It kinda drives me nuts how the `transformers` library hides so many details of the process via optional parameters which are set to default values.\n",
        ">\n",
        "> Worse, it often \"looks up\" the default value by referencing some model-specific code, which makes it hard to track down the actual values in the documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV8XZR8bBPG4"
      },
      "outputs": [],
      "source": [
        "# The tokenizer is a \"callable object\"--this invokes its __call__ function,\n",
        "# which will tokenize and encode all of the input strings.\n",
        "encodings = tokenizer(\n",
        "\n",
        "    labeled_sentences, # List of strings.\n",
        "\n",
        "    padding = 'longest',  # Pad out all of the samples to match the longest one\n",
        "                          # in the data.\n",
        "\n",
        "    #max_length = 64,      # An alternative strategy is to specify a maximum\n",
        "    #padding='max_length', # length, but it makes sense to let the tokenizer\n",
        "                           # figure that out.\n",
        "\n",
        "    truncation = False, # These samples are too short to need truncating. If we\n",
        "                        # did need to truncate, we'd also need to specify the\n",
        "                        # max_length above.\n",
        "\n",
        "    add_special_tokens = True, # Add the bos and eos tokens.\n",
        "\n",
        "    return_attention_mask = True, # To distinguish the padding tokens.\n",
        "\n",
        "    return_tensors = \"pt\" # Return the results as pytorch tensors.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw1by9T2NVa6"
      },
      "source": [
        "#### ‚öôÔ∏è Max Sequence Length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kji-r9WHcsl1"
      },
      "source": [
        "This is a critical number when it comes to the memory and compute requirements on the GPU.\n",
        "\n",
        "Compute is \"quadratic\" with sequence length (i.e., sequence length squared).\n",
        "\n",
        "Memory use would be quadratic, too, but some clever tricks have been found to avoid that, and now it's just linear.\n",
        "\n",
        "Still, going from, e.g., 64 tokens to 512 is going to make things 8x worse! üò≥\n",
        "\n",
        "With the prompt I've chosen, the maximum sequence length is 86 in the training data. On the T4, I just barely got a batch size of 8 to fit at this length.\n",
        "\n",
        "Also, note that training is much more memory-intensive than inference, so you can get away with a longer sequence length during inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym5Xl9a7kbGi",
        "outputId": "7e09ec97-d23a-47a9-9388-050f1223bd10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longest training sequence length: 86\n"
          ]
        }
      ],
      "source": [
        "max_seq_len = len(encodings['input_ids'][0])\n",
        "\n",
        "print(\"Longest training sequence length:\", max_seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__YgT6rLH4Pf"
      },
      "source": [
        "## 4.2. Define Targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9epDFfJ_6Ya"
      },
      "source": [
        "The tokenizer gave us the token ids and attention masks.\n",
        "\n",
        "For building our dataset, let's also store the locations of our label words.\n",
        "\n",
        "Also, in order to train an LLM on next token prediction, you give it both the input text and the desired output text.\n",
        "\n",
        "Typically these are identical--you're giving the model text and training it to reproduce it.\n",
        "\n",
        "Here, though, we don't need to mess with the model's ability to generate text in general--we just want to improve its ability to predict our label words in the appropriate spot.\n",
        "\n",
        "To do this, we set the token IDs for all of the output text to -100, _except_ for our label word.\n",
        "\n",
        "This sentinel value tells the code not to update the model for these tokens.\n",
        "\n",
        "It's not quite the same as an attention mask--we still want the model to process the input tokens so that the model can see and attend to them (i.e., pull information from them) during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdd9NBUHfMR3"
      },
      "outputs": [],
      "source": [
        "# I'll add the prefix 'all' to these variables, since they still contain both\n",
        "# the training and validation data.\n",
        "all_input_ids = []\n",
        "all_attention_masks = []\n",
        "all_target_words = []\n",
        "all_label_ids = []\n",
        "all_label_pos = []\n",
        "\n",
        "# For each of the encoded samples...\n",
        "for i in range(len(labels_as_ids)):\n",
        "\n",
        "    # Extract input_ids and attention_mask\n",
        "    input_ids = encodings['input_ids'][i]\n",
        "    attention_mask = encodings['attention_mask'][i]\n",
        "\n",
        "    # Find the position of the last non-padding token using the attention mask\n",
        "    # Because we appended the label to the end of the input, this is the\n",
        "    # position of our label word.\n",
        "    label_position = attention_mask.nonzero()[-1].item()\n",
        "\n",
        "    # This will tell the model what to token to predict at each position.\n",
        "    # (i.e., at position 12, the model should predict target_words[12])\n",
        "    # You can set the value to -100 for any tokens you don't want to train on,\n",
        "    # and in our case, we only want to train on the label.\n",
        "    # Start by filling it all out with -100s\n",
        "    target_words = torch.full_like(input_ids, -100)  # Initialize all labels to -100\n",
        "\n",
        "    # Get the token id for the label\n",
        "    label_id = labels_as_ids[i]\n",
        "\n",
        "    # We want all of the words / tokens masked out, except for the label.\n",
        "    target_words[label_position] = label_id\n",
        "\n",
        "    # Store everything.\n",
        "    all_input_ids.append(input_ids)\n",
        "    all_attention_masks.append(attention_mask)\n",
        "    all_target_words.append(target_words)\n",
        "    all_label_pos.append(label_position)\n",
        "    all_label_ids.append(label_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF7cEcEZOcYe"
      },
      "source": [
        "\n",
        "## 4.3. Split Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93Ox9mpwBnML"
      },
      "source": [
        "We'll split off 10% of our data to use as a validation set. By testing against the validation set periodically during training, we can create a plot at the end to show us whether the model was overfitting the training data. (i.e., performance on the training set kept improving, but performance on the validation set got worse)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeHcoGA_L3Ay",
        "outputId": "8c9b5c70-a5ae-43a7-951b-da7af91c41e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7,695 training samples\n",
            "  856 validation samples\n"
          ]
        }
      ],
      "source": [
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(all_input_ids))\n",
        "val_size = len(all_input_ids) - train_size\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu47TnEyCFnK"
      },
      "source": [
        "Combine all of the data into a TensorDataset object.\n",
        "\n",
        "Use torch.stack to convert the lists of vectors into matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_LPkfOkKpgq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Convert the lists into PyTorch tensors and put into a TensorDataset object\n",
        "\n",
        "dataset = TensorDataset(\n",
        "    # These variables are currently lists of vectors. `stack` will combine\n",
        "    # the vectors into a matrix with shape\n",
        "    #   8551 x 123 ([number of samples] x [sequence length])\n",
        "    torch.stack(all_input_ids),\n",
        "    torch.stack(all_attention_masks),\n",
        "    torch.stack(all_target_words),\n",
        "\n",
        "    # These are lists, and need to be vectors.\n",
        "    torch.tensor(all_label_ids),\n",
        "    torch.tensor(all_label_pos)\n",
        ")\n",
        "\n",
        "# For reference, this is how we'll unpack a batch:\n",
        "#     input_ids = batch[0]\n",
        "#     attn_masks = batch[1]\n",
        "#     targets = batch[2]\n",
        "#     label_ids = batch[3]\n",
        "#     label_pos = batch[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw4MqUF0uRMG"
      },
      "source": [
        "Before making the random split, let's set the seed value for consistent results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSTESUbUuKPP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GED6ZHeCXk6"
      },
      "source": [
        "Now we can use the `random_split` function from PyTorch to randomly shuffle our dataset, and then split it into the two parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyjjmhgTMA8i"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFyDciz5es7e"
      },
      "source": [
        "## 4.4. ‚öôÔ∏è Batch Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSEeLLJpvHvK"
      },
      "source": [
        "The final data preparation step is to wrap it in a PyTorch DataLoader, which will handle selecting batches for us. This means it's time to choose our training batch size!\n",
        "\n",
        "Our batch size can be a big issue for fitting training into a GPU's memory, because the memory use grows linearly with the batch size.\n",
        "\n",
        "Being forced to use smaller batches means less parallelism and slower training, but more importantly we miss out on the mathematical advantages and get a lower quality model!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSGjQH8NidXO"
      },
      "source": [
        "\n",
        "**Mathematical Batch vs. GPU Batch**\n",
        "\n",
        "However, things aren't actually quite that bleak, because the parallelism benefit and the mathematical benefit can be separated.\n",
        "\n",
        "The mathematical benefit of batching comes from averaging our weight update across multiple samples for a smoother training step. If we only have enough memory to process, i.e., 1 sample at a time on the GPU, that just means we have to run multiple forward passes and **accumulate the gradients** before we update the model weights.\n",
        "\n",
        "So really there are two separate parameters to choose:\n",
        "\n",
        "1. The **mathematical batch size** - What batch size do we want to use for averaging our updates?\n",
        "\n",
        "2. The **GPU batch size** - How many samples can we run in parrallel on the GPU without running out of memory.\n",
        "\n",
        "A third, implied parameter is the number of GPU passes to make per batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbeE_Bpj8wla",
        "outputId": "581ed740-64a1-46b8-f3bc-bcd55cc9cda9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For the math, we are using an (effective) batch size of 8.\n",
            "For memory constraints, we will only give the GPU 8 samples at a time.\n",
            "We'll do this by accumulating the gradients over 1 GPU passes before updating.\n"
          ]
        }
      ],
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here.\n",
        "\n",
        "# First, specify the mathematical batch size that we want.\n",
        "train_batch_size = 8\n",
        "\n",
        "# Second, specify the number of samples we want to give to the GPU at a time.\n",
        "# (We set this to a lower number if we're running out of memory)\n",
        "\n",
        "# Because our sequence length is so short, we can actually manage to use our\n",
        "# full batch size of 8 even on the 15GB T4--just barely!\n",
        "# If you try out a longer prompt, though, you may go over.\n",
        "if gpu == \"T4\" and max_seq_len > 90:\n",
        "    gpu_batch_size = 4\n",
        "# If you run this notebook as-is, then this size will work on all devices:\n",
        "else:\n",
        "    gpu_batch_size = 8\n",
        "\n",
        "# These must be evenly divisible.\n",
        "assert(train_batch_size % gpu_batch_size == 0)\n",
        "\n",
        "# Calculate how many batches to accumulate the gradients over.\n",
        "accumulate_passes = int(train_batch_size / gpu_batch_size)\n",
        "\n",
        "print(\"For the math, we are using an (effective) batch size \"\\\n",
        "      \"of {:}.\".format(train_batch_size))\n",
        "print(\"For memory constraints, we will only give the GPU {:} samples at \" \\\n",
        "      \"a time.\".format(gpu_batch_size))\n",
        "print(\"We'll do this by accumulating the gradients over {:} GPU \" \\\n",
        "      \"passes before updating.\".format(accumulate_passes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHILS33N7KFh"
      },
      "source": [
        "> <font color=\"olive\" size=\"-1\"><strong>Aside:</strong></font>\n",
        ">\n",
        "> <font size=\"-1\">If you use the HuggingFace Trainer class (which I think you should avoid! üòù), you'll see the mathematical batch size expressed indirectly as a combination of \"device batch size\" and \"batch accumulation steps\". I'm good with the first term, but not the second one... My biggest beef is that it confuses the definition of \"steps\"--using it to refer to GPU forward passes instead of actual optimizer/training steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upfqjfybT0gp"
      },
      "outputs": [],
      "source": [
        "# When running inference, there's no need for accumulation.\n",
        "val_batch_size = gpu_batch_size\n",
        "\n",
        "# The A100 can handle a bigger batch size to make the validation go quicker.\n",
        "if gpu == \"A100\":\n",
        "    val_batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGUqOCtgqGhP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# These will handle the creation of batches for us.\n",
        "# Note that we want to use the GPU batch size here, not the effective one.\n",
        "\n",
        "# We'll take training samples in random order.\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = gpu_batch_size\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = val_batch_size # Evaluate with this batch size.\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBJdcqML59me"
      },
      "source": [
        "Side Note: In this example, the dataset is plenty small enough that we can just load the whole thing into memory. For huge datasets, though, we have to leave them on disk, and the DataLoader also provides functionallity for loading just the current samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcRsi7c86ABm"
      },
      "source": [
        "# ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwa6Rts-02-"
      },
      "source": [
        "# S5. Compress & Load The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDifs5nBefTi"
      },
      "source": [
        "We're ready to load the model onto the GPU!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1G984Q7fS53"
      },
      "source": [
        "**bfloat16**\n",
        "\n",
        "One annoying little detail is that older architectures (including the T4) don't support the \"bfloat16\" data type, so we need to toggle between regular float16 and bfloat16 based on your GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ05BTj-uPw4"
      },
      "source": [
        "><font color='olive'><strong>Aside</strong></font>\n",
        ">\n",
        "> _The 'b' is for Google Brain, who introduced this data type specifically for deep learning._\n",
        ">\n",
        "> _Compared to float16, it can represent much huger and much tinier numbers, but with fewer distinct numbers in between._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKyxAAV1oxvc"
      },
      "outputs": [],
      "source": [
        "if gpu == \"T4\":\n",
        "    torch_dtype = torch.float16\n",
        "elif gpu == \"L4\" or gpu == \"A100\":\n",
        "    torch_dtype = torch.bfloat16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9WWktBXfSFl"
      },
      "source": [
        "**4-bit Quantization**\n",
        "\n",
        "To get our 8B parameter Llama 3.1 model to fit in memory, we're going to compress the weight values (\"quantize\" them).\n",
        "\n",
        "We'll use 4-bit quantization for this (from the \"QLoRA\" paper). Below is the configuration object for setting it up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_5sN74-fGx7"
      },
      "source": [
        "> <font color='olive'><strong>Aside</strong></font>\n",
        ">\n",
        "> _4-bit quantization is not the same as creating a 4-bit model--it's still 16-bit!_\n",
        ">\n",
        "> _Each parameter will be represented by a 4-bit value **multiplied by a 16-bit scaling factor** (stored to the side)._\n",
        ">\n",
        "> _That requires more memory, not less... The way we achieve compression is that every 64 parameters share the same 16-bit multiplier._\n",
        ">\n",
        "> _Kinda crazy, right? Check out my blog post [here](https://mccormickml.com/2024/09/14/qlora-and-4bit-quantization/) to learn how it works in-depth._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SV5an6-edUj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    # Enable 4-bit quantization.\n",
        "    load_in_4bit = True,\n",
        "\n",
        "    # With \"double quantization\" we not only compress the weights, we compress\n",
        "    # the scaling factors as well!\n",
        "    bnb_4bit_use_double_quant = False,\n",
        "\n",
        "    # The authors did some analysis of the distribution of weight values in\n",
        "    # popular transformer models, and chose some hardcoded values to use as the\n",
        "    # 16 \"base values\". They are normally distributed around 0.\n",
        "    # They refer to this as the \"nf4\" data type.\n",
        "    #\n",
        "    # The alternative choice is \"float4\", which only has 15 unique values, but\n",
        "    # also seems to be normally distributed.\n",
        "    bnb_4bit_quant_type = \"nf4\",\n",
        "\n",
        "    # The 16-bit data type for the scaling factors and math.\n",
        "    bnb_4bit_compute_dtype = torch_dtype\n",
        "\n",
        "    # Note that the \"block size\" of 64, which determines the compression ratio,\n",
        "    # doesn't appear to be configurable!\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23gKY_pf9k5_"
      },
      "outputs": [],
      "source": [
        "# Tell PyTorch to use the GPU.\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mXvTO2WDydc"
      },
      "source": [
        "Download the model and run the 4-bit compression algorithm on it.\n",
        "\n",
        "Note that, as discussed in the introduction, we're not using the \"AutoModelForSequenceClassification\" class, which adds a linear classifier. We're sticking with the original \"CausalLM\" class, which includes the LM head on the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377,
          "referenced_widgets": [
            "7994b237adf34ba4a17bb09c12e82293",
            "efc588c836c3467789107f07ac439a63",
            "a89c9223186f4cb9bf61a5397b5fb757",
            "ce5211faba2b4fe5be1be2234ee69510",
            "599c2ccb9ad542b5afc9e87a830e3229",
            "ba6c0f99cccc47048c7af6f4ef9d1aab",
            "c5131e49275447d1bc30007e5ffc9ea5",
            "6cbb4fe6700f4e39a27e402455ce2f84",
            "941c7294db704fd8806c38b5f53122d2",
            "4a89d6829b4b4f1184637f24423c8a5f",
            "811fdb60c51e4284b5d81246aba2a2e1",
            "2ce8e917df9d4de8b05428a71f3e2c34",
            "1caabf687bc34fa0a1ecafe36ab6ce5a",
            "3514fbccb2e24d8381222e57429c1905",
            "b5c9d89d2b52480188440dfff333466c",
            "56fc2f63070c422198186b8fbc305e19",
            "93f2063bb917485b87315a44d3ded4fe",
            "97383e78650f484892dad6d6cabc35a8",
            "b95ec09868a44c52a2b15f9c87f5319d",
            "be607af3d75e48ac90958122edf84e1b",
            "c565d6e63b354de3864fd9ecd032362c",
            "92eca86514d94b6faf8d0d8ca68af879",
            "5bb6b22c9020493a8996261e0e2f3e0f",
            "c272b96f8ff04302978a7375696d2585",
            "dfebcbc64f3a4db1bf858d79a6301240",
            "ba682f0940e6490985806850564f9324",
            "c83d36786f0d44699da2cc226e366f6d",
            "ceb4e27721d9400ab5139316fe4bc50d",
            "9d2d613e4d244c578e7f1faae1eb04be",
            "166671c5552c4fe69d3b520cd3c20ca6",
            "3b226e31455c4192bd6dd5c7ea860680",
            "237c97c0feea4ac1a1440e83525b83ca",
            "db21f4c419f14fe69ba6326228e953c8",
            "6f3ad9a56cc74b48903c6e06c9af90d8",
            "e7f0262a77e942008248717a8dc38e72",
            "e3e17d5392b84a529d4d8640d5edfd6d",
            "dcbfbc7f137a423d8a99e40daa23077e",
            "6442742f0797426f9a4c0966ba5efd44",
            "af2a47006c5b4d2cbd4d9504301019d6",
            "edac01c6d4434ab08baa976d3f9d1acd",
            "41d673eddc2246ec88421d3adfd32e7b",
            "867ab6eef96240ff829aa43170eca2be",
            "661cb899b91a4c1998b8d8c8d864d66d",
            "9e96009e0d1e4067af69bb97eda37296",
            "0038bdd3a2604b23a449402acf7783e6",
            "139b4baa7cc948428baaff4d05313f8a",
            "e4c7785373574f5c90d301b90d27cfbd",
            "768a7964725e41ca967e0c651fd2452a",
            "eac4e680cf7a4b33ade0623896d27ed5",
            "cd68eddd38514846816e622c2047b331",
            "412bd5a883414296ac942d4ac71b593f",
            "2b4f8a36cb6b4c13b7b37dd526cff806",
            "689dfac524bf4a08af176576c93f1c5b",
            "6069ab63d3f2426c910cc126ac1a1a78",
            "ed791d9f94ee45b2baa9c581ac1d3393",
            "3e32853ab3e044aeb6613f9937c258fc",
            "ccb1ebf2d24f485ab91188e4a1276e18",
            "ac6386392c7e43fdaafd2e9127fa0931",
            "61a449db0e224784ba89e7492466665d",
            "59543d512c2b4ce4a308723b41b22945",
            "046b5e87f0a148fabcaa450bf35f7b30",
            "82397fac6ae548439c88be7a51113b07",
            "b0ace65b65d94bfca7487d35eeb3943f",
            "8dd415f2c45046b7b5db22963e8b212c",
            "ffcb164625bf4013be19412fd44c4596",
            "0cf14fed4cfd49d798e3ce5adbe5ccc5",
            "7edafa5c7d614068a9e095b8be9cd164",
            "485ff9a93d274b4fb1321a75dfcd0197",
            "2fde1e66d5124620a876866ee1e14e38",
            "374a18e9c2014581bc59d45c937c0353",
            "12d1072820bf426cbd83243499e4ff59",
            "0cdcf8eedad443d090fa8d434a396b9f",
            "e059c97af9af4c8d8841b1aca85ff2eb",
            "67630f80bcaf48b6b192c2e210629ad0",
            "45634f5ca52f44d99ee66f0e9bcf50a8",
            "ba1de3629793438088412ccd79d47111",
            "6cbdaf2ee2b34f5aa7fd874af22fb76f",
            "f92a47e287a44eac8a5ef74a1f057ebb",
            "9b4fba2e53934d6ab250b3f7ecf868a8",
            "002b3022f00447158bbac0cfc4d2e557",
            "850d39ba957843a198344d2aabe8da5a",
            "9c8eb4b960a04acbbce2265849b19c56",
            "450f78d39afb407d83d169099980300e",
            "1da251b5598541f48ff894d7a3dab9dd",
            "750b2d829cae4546b73c5cf520ccf3b6",
            "517ef2f6710e443eb1c6c461b7ae27e5",
            "3b6192d160d64c349aca0a756606fdd4",
            "be5b0fc5b18945e584bd34031ae8ceb8",
            "aac2e57098094fa8a47fdb5fc4106f70",
            "7397fead1a02487fba4e0345524199ea",
            "bc5cc9a5d26e4937a3f40d36540db378",
            "789d14ded536464f918d4bdb9bb0d511",
            "6837604b81fe4646bdaed7e3572ae75e",
            "17604738a4ac4141b0cb896b2a26a39c",
            "85e16aba176c4bbf8122a4535b8cbeec",
            "6d36f5da28a3464aac48765abba44154",
            "dbe1894e43f449e3852883200d4ee8a3",
            "7d18766e76f544368d60a48c8779f4c8",
            "3081fbac79e94351a6ae95c5de07380e"
          ]
        },
        "id": "h19pI9if9lGh",
        "outputId": "fbd79119-cb09-4adc-cc76-09d37519bee0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7994b237adf34ba4a17bb09c12e82293",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ce8e917df9d4de8b05428a71f3e2c34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bb6b22c9020493a8996261e0e2f3e0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f3ad9a56cc74b48903c6e06c9af90d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0038bdd3a2604b23a449402acf7783e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e32853ab3e044aeb6613f9937c258fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7edafa5c7d614068a9e095b8be9cd164",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f92a47e287a44eac8a5ef74a1f057ebb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aac2e57098094fa8a47fdb5fc4106f70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading, compressing, and loading took 0:06:37\n",
            "\n",
            "GPU memory used to store model: 5.85 GB\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# Load the model.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B\",\n",
        "\n",
        "    # Our 4-bit quantization setup defined in the previous section.\n",
        "    quantization_config = bnb_config,\n",
        "\n",
        "    # Squared Dot-Product Attention is the generic name for the key innovation\n",
        "    # in FlashAttention. It's implemented in pytorch now. It's the default\n",
        "    # value for this parameter.\n",
        "    # You can change it to \"eager\" for the straightforward implementation if\n",
        "    # you want to compare the two!\n",
        "    attn_implementation = \"sdpa\",\n",
        "\n",
        "    # I assume it's critical that this datatype match the one used in\n",
        "    # the quantization configuration, and in LoRA!\n",
        "    torch_dtype = torch_dtype,\n",
        "\n",
        "    # I was getting this output message:\n",
        "    #    \"`low_cpu_mem_usage` was None, now set to True since model is\n",
        "    #      quantized.\"\n",
        "    # So I'm setting it to \"True\" as the message suggests. :)\n",
        "    #\n",
        "    # I tried setting it to False out of curiousity, and it crashed with:\n",
        "    #    \"Your session crashed after using all available RAM.\"\n",
        "    low_cpu_mem_usage = True,\n",
        "\n",
        "    # The model needs to know that we've set a pad token.\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "# Typically I would tell the model to run on the GPU at this point, but this\n",
        "# actually throws an error:\n",
        "#   \"ValueError: Calling `cuda()` is not supported for `4-bit` or `8-bit`\n",
        "#    quantized models. Please use the model as it is, since the model has\n",
        "#    already been set to the correct devices and casted to the correct `dtype`.\"\n",
        "#model.cuda()\n",
        "\n",
        "print(\"\\nDownloading, compressing, and loading took\", format_time(time.time() - t0))\n",
        "\n",
        "gpu_mem_model = gpu_mem_used()\n",
        "\n",
        "print(\"\\nGPU memory used to store model: {:}\".format(gpu_mem_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXUnQHXeOI3E"
      },
      "source": [
        "It's kinda interesting to check out the parameters after compression. The matrices get unrolled into 1D vectors, and two 4-bit values are packed into each byte, so the parameter counts are no longer accurate (they've been cut in half).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPdOprYUOI9f",
        "outputId": "7bf4da40-82c2-44ac-9273-2e1faa8561cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 291 different named parameters.\n",
            "\n",
            "Parameter Name                                              Dimensions       Total Values    Trainable\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "model.embed_tokens.weight                                  128,256 x 4,096           501M    True\n",
            "\n",
            "==== First Decoder ====\n",
            "\n",
            "model.layers.0.self_attn.q_proj.weight                   8,388,608 x 1                 8M    False\n",
            "model.layers.0.self_attn.k_proj.weight                   2,097,152 x 1                 2M    False\n",
            "model.layers.0.self_attn.v_proj.weight                   2,097,152 x 1                 2M    False\n",
            "model.layers.0.self_attn.o_proj.weight                   8,388,608 x 1                 8M    False\n",
            "model.layers.0.mlp.gate_proj.weight                     29,360,128 x 1                28M    False\n",
            "model.layers.0.mlp.up_proj.weight                       29,360,128 x 1                28M    False\n",
            "model.layers.0.mlp.down_proj.weight                     29,360,128 x 1                28M    False\n",
            "model.layers.0.input_layernorm.weight                        4,096 x -                 4K    True\n",
            "model.layers.0.post_attention_layernorm.weight               4,096 x -                 4K    True\n",
            "\n",
            "==== Second Decoder ====\n",
            "\n",
            "model.layers.1.self_attn.q_proj.weight                   8,388,608 x 1                 8M    False\n",
            "model.layers.1.self_attn.k_proj.weight                   2,097,152 x 1                 2M    False\n",
            "model.layers.1.self_attn.v_proj.weight                   2,097,152 x 1                 2M    False\n",
            "model.layers.1.self_attn.o_proj.weight                   8,388,608 x 1                 8M    False\n",
            "model.layers.1.mlp.gate_proj.weight                     29,360,128 x 1                28M    False\n",
            "model.layers.1.mlp.up_proj.weight                       29,360,128 x 1                28M    False\n",
            "model.layers.1.mlp.down_proj.weight                     29,360,128 x 1                28M    False\n",
            "model.layers.1.input_layernorm.weight                        4,096 x -                 4K    True\n",
            "model.layers.1.post_attention_layernorm.weight               4,096 x -                 4K    True\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "model.norm.weight                                            4,096 x -                 4K    True\n",
            "lm_head.weight                                             128,256 x 4,096           501M    True\n"
          ]
        }
      ],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print(\"Parameter Name                                              Dimensions       Total Values    Trainable\\n\")\n",
        "\n",
        "# For the first xx named parameters...\n",
        "for i in range(len(params)):\n",
        "\n",
        "    # First param is the embeddings\n",
        "    if i == 0:\n",
        "        print('==== Embedding Layer ====\\n')\n",
        "\n",
        "    # Next 10 parameters are the first decoder layer.\n",
        "    elif i == 1:\n",
        "        print('\\n==== First Decoder ====\\n')\n",
        "\n",
        "    # Next 10 parameters are the second decoder layer.\n",
        "    elif i == 10:\n",
        "        print('\\n==== Second Decoder ====\\n')\n",
        "\n",
        "    # Skip the other layers\n",
        "    elif i >= 19 and i < len(params)-2:\n",
        "        continue\n",
        "\n",
        "    # Final 2 are the output layer.\n",
        "    elif i == len(params)-2:\n",
        "        print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "    # Get the name and the parameter.\n",
        "    p_name, p = params[i]\n",
        "\n",
        "    # Strip some unnecessary prefixes.\n",
        "    if 'base_model.model.model.' in p_name:\n",
        "        p_name = p_name[len('base_model.model.'):]\n",
        "\n",
        "\n",
        "    # For 1D parameters, put '-' as the second dimension.\n",
        "    if len(p.size()) == 1:\n",
        "        p_dims = \"{:>10,} x {:<10}\".format(p.size()[0], \"-\")\n",
        "\n",
        "    # For 2D parameters...\n",
        "    if len(p.size()) == 2:\n",
        "        p_dims = \"{:>10,} x {:<10,}\".format(p.size()[0], p.size()[1])\n",
        "\n",
        "    # Print the parameter name, shape, number of elements, and whether it's been\n",
        "    # 'frozen' or not.\n",
        "    print(\"{:<55} {:}    {:>6}    {:}\".format(p_name, p_dims, format_size(p.numel()), p.requires_grad))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkfL0NEtenhI"
      },
      "source": [
        "Note that the 16-bit scaling factors are stored separately from the model, so they're not reflected here.\n",
        "\n",
        "Also, quantization has _not_ been applied to the input embeddings or to the output LM head. We could still fine-tune these if we wanted to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBhn0FV0C96Q"
      },
      "source": [
        "For comparison, here are those same parameters if you don't apply quantization\n",
        "\n",
        "```\n",
        "The model has 291 different named parameters.\n",
        "\n",
        "Parameter Name                                              Dimensions       Total Values    Trainable\n",
        "\n",
        "==== Embedding Layer ====\n",
        "\n",
        "model.embed_tokens.weight                                  128,256 x 4,096           501M    True\n",
        "\n",
        "==== First Decoder ====\n",
        "\n",
        "model.layers.0.self_attn.q_proj.weight                       4,096 x 4,096            16M    True\n",
        "model.layers.0.self_attn.k_proj.weight                       1,024 x 4,096             4M    True\n",
        "model.layers.0.self_attn.v_proj.weight                       1,024 x 4,096             4M    True\n",
        "model.layers.0.self_attn.o_proj.weight                       4,096 x 4,096            16M    True\n",
        "model.layers.0.mlp.gate_proj.weight                         14,336 x 4,096            56M    True\n",
        "model.layers.0.mlp.up_proj.weight                           14,336 x 4,096            56M    True\n",
        "model.layers.0.mlp.down_proj.weight                          4,096 x 14,336           56M    True\n",
        "model.layers.0.input_layernorm.weight                        4,096 x -                 4K    True\n",
        "model.layers.0.post_attention_layernorm.weight               4,096 x -                 4K    True\n",
        "\n",
        "==== Second Decoder ====\n",
        "\n",
        "model.layers.1.self_attn.q_proj.weight                       4,096 x 4,096            16M    True\n",
        "model.layers.1.self_attn.k_proj.weight                       1,024 x 4,096             4M    True\n",
        "model.layers.1.self_attn.v_proj.weight                       1,024 x 4,096             4M    True\n",
        "model.layers.1.self_attn.o_proj.weight                       4,096 x 4,096            16M    True\n",
        "model.layers.1.mlp.gate_proj.weight                         14,336 x 4,096            56M    True\n",
        "model.layers.1.mlp.up_proj.weight                           14,336 x 4,096            56M    True\n",
        "model.layers.1.mlp.down_proj.weight                          4,096 x 14,336           56M    True\n",
        "model.layers.1.input_layernorm.weight                        4,096 x -                 4K    True\n",
        "model.layers.1.post_attention_layernorm.weight               4,096 x -                 4K    True\n",
        "\n",
        "==== Output Layer ====\n",
        "\n",
        "model.norm.weight                                            4,096 x -                 4K    True\n",
        "lm_head.weight                                             128,256 x 4,096\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWaUtzhj7FH3"
      },
      "source": [
        "# S6. Evaluation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cofDr3W9Gye8"
      },
      "source": [
        "Let's define a function for evaluating our model on validation or test data.\n",
        "\n",
        "We'll use this function in three places:\n",
        "\n",
        "1. To test our performance prior to any fine-tuning.\n",
        "2. To run validation during training.\n",
        "3. To evaluate on the test set after training.\n",
        "\n",
        "This is where we're implementing our classification strategy--we look at the logit values for just our two label words and compare these to make a prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEaSZDlB7OXk"
      },
      "source": [
        "> <font color=\"olive\" size=\"-1\"><strong>Aside:</strong></font>\n",
        ">\n",
        "><font size=\"-1\">I typically try to avoid splitting off important functionality into functions. You get a better sense of what's really going on when it's all laid out in one place (so you're not having to jump around to remember what different functions do). That may mean maintaining multiple copies of the same code, and yes, that can introduce errors when you forget to fix the copies--but misunderstanding the code can certainly introduce errors as well! In this case, it's a pretty big block of code, and I think it's conceptually distinct enough from the rest of the training loop that I don't feel too bad about separating it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Fo7wFeR7HDP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(model, validation_dataloader):\n",
        "    \"\"\"\n",
        "    Run the model against a validation or test set and return a list of true\n",
        "    labels and a list of predicted labels.\n",
        "    \"\"\"\n",
        "\n",
        "    #t0 = time.time()\n",
        "\n",
        "    batch_num = 0\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Gather the true labels and predictions\n",
        "    true_labels = []\n",
        "    predictions = []\n",
        "\n",
        "    # For each batch...\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Print progress\n",
        "        if batch_num % 40 == 0:\n",
        "            print(\"  Batch {:>5,}  of  {:>5,}.\".format(batch_num, len(validation_dataloader)))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "        # the `to` method.\n",
        "\n",
        "        # These three all have the same shape of [batch_size x sequence_length]\n",
        "        # e.g., [8 x 128]\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        targets = batch[2].to(device)\n",
        "        true_label_ids = batch[3].to(device) # Shape: [8] (batch_size)\n",
        "        label_positions = batch[4].to(device) # Shape: [8] (batch_size)\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Get the model's predictions\n",
        "            outputs = model(\n",
        "                input_ids = input_ids,\n",
        "                attention_mask = attention_mask,\n",
        "                labels = targets\n",
        "            )\n",
        "\n",
        "        # The output predictions for every token in the input.\n",
        "        # Shape: [8, 128, vocab_size] (batch_size, sequence_length, vocab_size)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Extract the predicted token for each sample in the batch, one at a time.\n",
        "        # For each sample in the batch...\n",
        "        for i in range(input_ids.shape[0]):\n",
        "\n",
        "            # The index of the final (non-padding) token.\n",
        "            label_position = label_positions[i].item()\n",
        "\n",
        "            # Extract logits for the prediction.\n",
        "            # The logits for the position *just before* the label are the\n",
        "            # predictions we want.\n",
        "            # Shape: [vocab_size]\n",
        "            label_logits = logits[i, label_position - 1]\n",
        "\n",
        "            # Make our prediction by comparing the confidence for the two\n",
        "            # label words.\n",
        "            if label_logits[pos_token_id] > label_logits[neg_token_id]:\n",
        "                predicted_token_id = pos_token_id\n",
        "            else:\n",
        "                predicted_token_id = neg_token_id\n",
        "\n",
        "            # Append the true and predicted token IDs for comparison\n",
        "            true_labels.append(true_label_ids[i].item())\n",
        "            predictions.append(predicted_token_id)\n",
        "\n",
        "        # Increment the batch counter\n",
        "        batch_num += 1\n",
        "\n",
        "    # Report the average validation loss, which we can use to compare to our\n",
        "    # training loss and detect overfitting.\n",
        "    avg_loss = total_loss / batch_num\n",
        "\n",
        "    #print(\"\\n  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    return(true_labels, predictions, avg_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkpTzFL3-23n"
      },
      "source": [
        "## 6.1. Few-Shot Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W89ODR7LHtyc"
      },
      "source": [
        "Because of our few-shot prompt, our model ought to perform fairly well on the task without any fine-tuning. Let's measure it's performance on the validation set as a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuP1AWi-8llX",
        "outputId": "cff03f66-2885-481a-dffa-48fe7c9ed5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation...\n",
            "  Batch     0  of    107.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Batch    40  of    107.\n",
            "  Batch    80  of    107.\n"
          ]
        }
      ],
      "source": [
        "print('Running validation...')\n",
        "\n",
        "true_labels, predictions, avg_loss = evaluate(model, validation_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlkEsyXhILJW"
      },
      "source": [
        "Because of the class imbalance, the \"flat accuracy\" (num right / num samples) isn't a great metric.\n",
        "\n",
        "The official metric for the benchmark is the Matthews Correlation Coefficient (MCC)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnty7aG7fYqI",
        "outputId": "4b766e25-190d-4374-f800-f73f6b53a0b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flat accuracy: 78.86%\n",
            "\n",
            "Confusion Matrix:\n",
            "\n",
            "[[463 133]\n",
            " [ 48 212]]\n",
            "\n",
            "  TP    FP\n",
            "  FN    TN\n",
            "\n",
            "MCC on the validation set without fine-tuning: 0.555\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "true_labels = np.asarray(true_labels)\n",
        "predictions = np.asarray(predictions)\n",
        "\n",
        "# Report the final accuracy for this validation run.\n",
        "val_accuracy = float(np.sum(true_labels == predictions)) / len(true_labels) * 100.0\n",
        "print(\"Flat accuracy: {0:.2f}%\".format(val_accuracy))\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Print or log the confusion matrix\n",
        "print(\"\\nConfusion Matrix:\\n\")\n",
        "print(conf_matrix)\n",
        "print(\"\")\n",
        "print(\"  TP    FP\")\n",
        "print(\"  FN    TN\")\n",
        "\n",
        "# Measure the MCC on the validation set.\n",
        "few_shot_mcc = matthews_corrcoef(true_labels, predictions)\n",
        "\n",
        "print(\"\\nMCC on the validation set without fine-tuning: {0:.3f}\".format(few_shot_mcc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFydRsWMCau1"
      },
      "source": [
        "In a separate Notebook, I also ran this few-shot performance on the test set.\n",
        "\n",
        "* Without 4-bit quantization: MCC = `0.508`\n",
        "* With 4-bit: `0.495`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ckwp-o39lPK"
      },
      "source": [
        "Let's see how much memory it required to run some forward passes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvBWYyxG9vi0",
        "outputId": "e2f644ee-f873-412f-915e-653f3b98368b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GPU memory used for forward passes: 7.59 GB\n",
            "(With GPU batch size 8 and sequence length 86)\n"
          ]
        }
      ],
      "source": [
        "# Get the current GPU memory, which will reflect how much memory is required\n",
        "# to store the activation values.\n",
        "gpu_mem_forward_pass = gpu_mem_used()\n",
        "\n",
        "print(\"\\nGPU memory used for forward passes: {:}\".format(gpu_mem_forward_pass))\n",
        "print(\n",
        "    \"(With GPU batch size {:} and sequence length {:})\".format(val_batch_size,\n",
        "                                                               len(encodings['input_ids'][0]))\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAA5pkVPk_ru"
      },
      "source": [
        "# S7. Adding LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb5DY0vIWWJe"
      },
      "source": [
        "The compressed model values can't be trained--any updates to the values would require re-running the compression algorithm.\n",
        "\n",
        "LoRA adds on a small number of additional weights that we _can_ train.\n",
        "\n",
        "But more importantly, LoRA also serves as a form of \"regularization\"--it limits how much we can change the model during fine-tuning. This is important for avoiding over-fitting our small training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8LnG1iyZ-bQ"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# This is the key parameter. Think of it like the number of neurons you're\n",
        "# using in each hidden layer of a neural network. More will allow for\n",
        "# bigger changes to the model behavior but require more training data.\n",
        "lora_rank = 8\n",
        "\n",
        "# LoRA multiplies the gradients by a scaling factor, s = alpha / rank.\n",
        "# When you double the rank, the size gradients of the gradients tend to double\n",
        "# as well, so `s` is used to scale them back down.\n",
        "# If you want to play with different rank values, you can keep alpha constant in\n",
        "# order to maintain the same (effective) scaling factor.\n",
        "#\n",
        "# An oddity here is that `s` is redundant with the learning rate... both are\n",
        "# just multipliers applied to the gradients.\n",
        "# They could have addressed this instead by saying, e.g., \"when doubling the\n",
        "# rank, it's suggested that you cut the learning rate in half\"\n",
        "lora_alpha = 16\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r = lora_rank,\n",
        "\n",
        "    lora_alpha = lora_alpha,\n",
        "\n",
        "    lora_dropout = 0.05,\n",
        "\n",
        "    # The original convention with LoRA was to apply it to just the Query and\n",
        "    # Value matrices. An important contribution of the QLoRA paper was that it's\n",
        "    # important (and not very costly) to apply it to \"everything\" (the attention\n",
        "    # matrices and the FFN)\n",
        "    #target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    bias=\"none\",\n",
        "\n",
        "    # This is a crucial parameter that I missed when initially trying to use the\n",
        "    # LlamaForSequenceClassification class. I couldn't figure out why my model\n",
        "    # wasn't learning, until I discovered that LoRA was freezing the parameters\n",
        "    # of the linear classifier head!\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k4-X8L5_yU7",
        "outputId": "1b06563e-b796-48b2-bfa7-d6baddcf42a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096, padding_idx=128001)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from peft import get_peft_model\n",
        "\n",
        "# Wrap with PEFT Model applying LoRA\n",
        "model = get_peft_model(model, lora_config).to(device)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grvjxsGvFSOA"
      },
      "outputs": [],
      "source": [
        "# This is a bad idea / doesn't work when mixed with quantization.\n",
        "# Also misleading about what LoRA is for.\n",
        "#model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PbT7o1WKAgu",
        "outputId": "c687c08c-98e0-4169-e772-f4cd846e831d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GPU memory used after adding LoRA: 7.67 GB\n"
          ]
        }
      ],
      "source": [
        "gpu_mem_lora = gpu_mem_used()\n",
        "print(\"\\nGPU memory used after adding LoRA: {:}\".format(gpu_mem_lora))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Jv6c7-HHDW"
      },
      "source": [
        "Let's peek at the architecture again now that LoRA's been added.\n",
        "\n",
        "We can see all of the A, B matrices added by LoRA. They have the dimensions of the embedding size and our chosen rank.\n",
        "\n",
        "Note that by default LoRA freezes the embeddings and the LM head.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTyMMcCADpVM",
        "outputId": "ccd0e677-bb4f-4315-80fc-cc3d34764134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 739 different named parameters.\n",
            "\n",
            "Parameter Name                                              Dimensions       Total Values    Trainable\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "model.embed_tokens.weight                                  128,256 x 4,096           501M    False\n",
            "\n",
            "==== First Decoder ====\n",
            "\n",
            "model.layers.0.self_attn.q_proj.base_layer.weight        8,388,608 x 1                 8M    False\n",
            "model.layers.0.self_attn.q_proj.lora_A.default.weight            8 x 4,096            32K    True\n",
            "model.layers.0.self_attn.q_proj.lora_B.default.weight        4,096 x 8                32K    True\n",
            "model.layers.0.self_attn.k_proj.base_layer.weight        2,097,152 x 1                 2M    False\n",
            "model.layers.0.self_attn.k_proj.lora_A.default.weight            8 x 4,096            32K    True\n",
            "model.layers.0.self_attn.k_proj.lora_B.default.weight        1,024 x 8                 8K    True\n",
            "model.layers.0.self_attn.v_proj.base_layer.weight        2,097,152 x 1                 2M    False\n",
            "model.layers.0.self_attn.v_proj.lora_A.default.weight            8 x 4,096            32K    True\n",
            "model.layers.0.self_attn.v_proj.lora_B.default.weight        1,024 x 8                 8K    True\n",
            "model.layers.0.self_attn.o_proj.base_layer.weight        8,388,608 x 1                 8M    False\n",
            "model.layers.0.self_attn.o_proj.lora_A.default.weight            8 x 4,096            32K    True\n",
            "model.layers.0.self_attn.o_proj.lora_B.default.weight        4,096 x 8                32K    True\n",
            "model.layers.0.mlp.gate_proj.base_layer.weight          29,360,128 x 1                28M    False\n",
            "model.layers.0.mlp.gate_proj.lora_A.default.weight               8 x 4,096            32K    True\n",
            "model.layers.0.mlp.gate_proj.lora_B.default.weight          14,336 x 8               112K    True\n",
            "model.layers.0.mlp.up_proj.base_layer.weight            29,360,128 x 1                28M    False\n",
            "model.layers.0.mlp.up_proj.lora_A.default.weight                 8 x 4,096            32K    True\n",
            "model.layers.0.mlp.up_proj.lora_B.default.weight            14,336 x 8               112K    True\n",
            "model.layers.0.mlp.down_proj.base_layer.weight          29,360,128 x 1                28M    False\n",
            "model.layers.0.mlp.down_proj.lora_A.default.weight               8 x 14,336          112K    True\n",
            "model.layers.0.mlp.down_proj.lora_B.default.weight           4,096 x 8                32K    True\n",
            "model.layers.0.input_layernorm.weight                        4,096 x -                 4K    False\n",
            "model.layers.0.post_attention_layernorm.weight               4,096 x -                 4K    False\n",
            "\n",
            "==== Second Decoder ====\n",
            "\n",
            "model.layers.1.self_attn.q_proj.base_layer.weight        8,388,608 x 1                 8M    False\n",
            "model.layers.1.self_attn.q_proj.lora_A.default.weight            8 x 4,096            32K    True\n",
            "model.layers.1.self_attn.q_proj.lora_B.default.weight        4,096 x 8                32K    True\n",
            "model.layers.1.self_attn.k_proj.base_layer.weight        2,097,152 x 1                 2M    False\n",
            "model.layers.1.self_attn.k_proj.lora_A.default.weight            8 x 4,096            32K    True\n",
            "model.layers.1.self_attn.k_proj.lora_B.default.weight        1,024 x 8                 8K    True\n",
            "model.layers.1.self_attn.v_proj.base_layer.weight        2,097,152 x 1                 2M    False\n",
            "model.layers.1.self_attn.v_proj.lora_A.default.weight            8 x 4,096            32K    True\n",
            "model.layers.1.self_attn.v_proj.lora_B.default.weight        1,024 x 8                 8K    True\n",
            "model.layers.1.self_attn.o_proj.base_layer.weight        8,388,608 x 1                 8M    False\n",
            "model.layers.1.self_attn.o_proj.lora_A.default.weight            8 x 4,096            32K    True\n",
            "model.layers.1.self_attn.o_proj.lora_B.default.weight        4,096 x 8                32K    True\n",
            "model.layers.1.mlp.gate_proj.base_layer.weight          29,360,128 x 1                28M    False\n",
            "model.layers.1.mlp.gate_proj.lora_A.default.weight               8 x 4,096            32K    True\n",
            "model.layers.1.mlp.gate_proj.lora_B.default.weight          14,336 x 8               112K    True\n",
            "model.layers.1.mlp.up_proj.base_layer.weight            29,360,128 x 1                28M    False\n",
            "model.layers.1.mlp.up_proj.lora_A.default.weight                 8 x 4,096            32K    True\n",
            "model.layers.1.mlp.up_proj.lora_B.default.weight            14,336 x 8               112K    True\n",
            "model.layers.1.mlp.down_proj.base_layer.weight          29,360,128 x 1                28M    False\n",
            "model.layers.1.mlp.down_proj.lora_A.default.weight               8 x 14,336          112K    True\n",
            "model.layers.1.mlp.down_proj.lora_B.default.weight           4,096 x 8                32K    True\n",
            "model.layers.1.input_layernorm.weight                        4,096 x -                 4K    False\n",
            "model.layers.1.post_attention_layernorm.weight               4,096 x -                 4K    False\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "model.norm.weight                                            4,096 x -                 4K    False\n",
            "base_model.model.lm_head.weight                            128,256 x 4,096           501M    False\n"
          ]
        }
      ],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print(\"Parameter Name                                              Dimensions       Total Values    Trainable\\n\")\n",
        "\n",
        "# For the first xx named parameters...\n",
        "for i in range(len(params)):\n",
        "\n",
        "    # First param is the embeddings\n",
        "    if i == 0:\n",
        "        print('==== Embedding Layer ====\\n')\n",
        "\n",
        "    # Next 24 parameters are the first decoder layer.\n",
        "    elif i == 1:\n",
        "        print('\\n==== First Decoder ====\\n')\n",
        "\n",
        "    # Next 24 parameters are the second decoder layer.\n",
        "    elif i == 24:\n",
        "        print('\\n==== Second Decoder ====\\n')\n",
        "\n",
        "    # Skip the other layers\n",
        "    elif i > 46 and i < len(params)-2:\n",
        "        continue\n",
        "\n",
        "    # Final 2 are the output layer.\n",
        "    elif i == len(params)-2:\n",
        "        print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "    # Get the name and the parameter.\n",
        "    p_name, p = params[i]\n",
        "\n",
        "    # Strip some unnecessary prefixes.\n",
        "    if 'base_model.model.model.' in p_name:\n",
        "        p_name = p_name[len('base_model.model.'):]\n",
        "\n",
        "    # For 1D parameters, put '-' as the second dimension.\n",
        "    if len(p.size()) == 1:\n",
        "        p_dims = \"{:>10,} x {:<10}\".format(p.size()[0], \"-\")\n",
        "\n",
        "    # For 2D parameters...\n",
        "    if len(p.size()) == 2:\n",
        "        p_dims = \"{:>10,} x {:<10,}\".format(p.size()[0], p.size()[1])\n",
        "\n",
        "    # Print the parameter name, shape, number of elements, and whether it's been\n",
        "    # 'frozen' or not.\n",
        "    print(\"{:<55} {:}    {:>6}    {:}\".format(p_name, p_dims, format_size(p.numel()), p.requires_grad))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEt1UcNp5x2c"
      },
      "source": [
        "# ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI4LiH1eKDox"
      },
      "source": [
        "# S8. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWT-D4U_Pvx"
      },
      "source": [
        "## 8.1. Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnRNqBf5OtYU"
      },
      "source": [
        "### ‚öôÔ∏è Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcUf6FKAyBmk"
      },
      "source": [
        "The number of different settings involved in training can be overwhelming, but you definitely want to pay attention to the **learning rate** and **batch size**. These can have a big impact on performance, and you should experiment with them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgoVr9iz6wJp"
      },
      "source": [
        "**What is the Learning Rate?**\n",
        "\n",
        "As part of training, the calculated **weight updates** are **scaled down**(dramatically!) by multiplying them with the learning rate.\n",
        "\n",
        "In general, smaller batch sizes need smaller learning rates, and vice versa:\n",
        "\n",
        "* Roughly speaking, a larger batch size can produce more accurate weight updates--you're averaging the updates over more samples in order to make a more educated guess about which direction to move in.\n",
        "\n",
        "* Larger learning rates (e.g., 1e-4) mean bigger changes to the model at each step. And if your weight updates are more accurate (from averaging over a larger batch), then it's safe to be more aggressive (with a higher learning rate) in making changes.\n",
        "\n",
        "...but you can also end up watering down the influence of each training sample with too large of a batch size.\n",
        "\n",
        "So, in the end, it's best to just sweep over a bunch of possible combinations! üòù\n",
        "\n",
        "I didn't do a thorough parameter sweep, but a batch size of 8 and a learning rate of 1e-4 gave me the best results out of what I tried."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1XtHULF66S-"
      },
      "source": [
        "**What's an 8-bit Optimizer?**\n",
        "\n",
        "The Adam optimizer applies weight updates in a more complicated way than the simple update function I learned in Machine Learning 101.\n",
        "\n",
        "The math's a bit complicated, but roughly speaking, it adjusts the weight updates with information from prior updates in order to smooth things out overall.\n",
        "\n",
        "It needs to store multiple additional values for every parameter in the model, and the 8-bit optimizer applies a compression technique to store these.\n",
        "\n",
        "Just like the 4-bit quantization we applied to our model earlier, this is _not the same_ as dropping the optimizer precision to 8-bits--there is additional higher precision metadata stored in order to return the values to (16-bits?) before they're used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxV4C3UfKpL2"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes as bnb\n",
        "from transformers import AdamW\n",
        "\n",
        "# Note that the LoRA ratio has a linear impact on the lr, so with 16/8,\n",
        "# 1e-4 is effectively 2e-4.\n",
        "lr = 1e-4\n",
        "\n",
        "# At these lower sequence lengths, the 8-bit optimizer makes a significant\n",
        "# difference.\n",
        "if gpu == 'T4':\n",
        "    #optimizer_class = bnb.optim.PagedAdamW8bit\n",
        "    optimizer_class = bnb.optim.AdamW8bit\n",
        "else:\n",
        "    optimizer_class = AdamW\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = optimizer_class(\n",
        "\n",
        "    model.parameters(), # The optimizer is responsible for making the updates to\n",
        "                        # to our model parameters.\n",
        "    lr = lr,\n",
        "\n",
        "    weight_decay = 0.05, # Weight decay (related to regularization)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4R3MEu00kQh"
      },
      "source": [
        "The learning rate scheduler is responsible for gradually decreasing the learning rate over the course of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "import math\n",
        "\n",
        "# len(train_dataloader) gives us the number of GPU batches the data loader will\n",
        "# divide our dataset into.\n",
        "num_gpu_batches = len(train_dataloader)\n",
        "\n",
        "# We need to divide the number of GPU batches by the number of accumulation\n",
        "# passes to get the true number of update steps.\n",
        "# Round up since there may be one partial batch at the end.\n",
        "num_update_steps = math.ceil(num_gpu_batches / accumulate_passes)\n",
        "\n",
        "# Our LoRA weights are completely random at first, so the initial gradients will\n",
        "# be large. To counter this, we do some warmup steps with a tiny learning rate,\n",
        "# gradually building up to the actual lr. Then the scheduler takes over.\n",
        "num_warmup_steps = 100\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = num_warmup_steps,\n",
        "    num_training_steps = num_update_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg7eMHQZZQLp"
      },
      "outputs": [],
      "source": [
        "# Without batch accumulation...\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "#total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "#scheduler = get_linear_schedule_with_warmup(\n",
        "#    optimizer,\n",
        "#    num_warmup_steps = 100,\n",
        "#    num_training_steps = total_steps\n",
        "#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfmWwUR_Sox"
      },
      "source": [
        "## 8.2. Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QXZhFb4LnV5"
      },
      "source": [
        "Below is our training loop. There's a lot going on, but here's the summary:\n",
        "\n",
        "**Training:**\n",
        "\n",
        "We take one batch at a time from the data loader, move it to the GPU, and execute:\n",
        "\n",
        "1. `forward()` - Run the samples through the model to get predictions.\n",
        "\n",
        "2. `loss.item()` - Retrieve the \"loss\", a measure of how poorly the model did on these samples.\n",
        "3. `backward()` - Starting from the output, work backwards to calculate how much each parameter contributed to the loss, (the \"deltas\") and the derivative of those deltas (the \"gradients\").\n",
        "4. `optimizer.step()` - Update the weights using the Adam formula by combining the learning rate, gradients, and those other stored smoothing values.\n",
        "\n",
        "**Validation:**\n",
        "\n",
        "Periodically, we'll check to see how we're doing on that 10% of the data we set aside for validation.\n",
        "\n",
        "If we're continuing to improve on the training data, but see our performance on the validation set start to decrease, then we know we're starting to overfit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfNIhN19te3N"
      },
      "source": [
        "We're ready to kick off the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J-FYdx6nFE_",
        "outputId": "2bcf3941-9241-493d-d3d3-12c618081cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "'step' refers to one optimizer step (an important distinction if you're using gradient accumulation).\n",
            "\n",
            "      Step    of Total     Elapsed          Loss     GPU Mem            lr\n",
            "        10         962     0:00:19        0.6069    14.67 GB    1.0 x 1e-5\n",
            "        20         962     0:00:38        0.5748    14.67 GB    2.0 x 1e-5\n",
            "        30         962     0:00:57        0.5114    14.67 GB    3.0 x 1e-5\n",
            "        40         962     0:01:16        0.4691    14.67 GB    4.0 x 1e-5\n",
            "        50         962     0:01:35        0.5011    14.67 GB    5.0 x 1e-5\n",
            "        60         962     0:01:53        0.4732    14.67 GB    6.0 x 1e-5\n",
            "        70         962     0:02:13        0.4755    14.67 GB    7.0 x 1e-5\n",
            "        80         962     0:02:31        0.4674    14.67 GB    8.0 x 1e-5\n",
            "        90         962     0:02:50        0.5023    14.67 GB    9.0 x 1e-5\n",
            "       100         962     0:03:09        0.4910    14.67 GB   10.0 x 1e-5\n",
            "       110         962     0:03:28        0.5025    14.67 GB    9.9 x 1e-5\n",
            "       120         962     0:03:47        0.4897    14.67 GB    9.8 x 1e-5\n",
            "       130         962     0:04:06        0.4970    14.67 GB    9.7 x 1e-5\n",
            "       140         962     0:04:25        0.4870    14.67 GB    9.5 x 1e-5\n",
            "       150         962     0:04:44        0.4844    14.67 GB    9.4 x 1e-5\n",
            "       160         962     0:05:03        0.4843    14.67 GB    9.3 x 1e-5\n",
            "       170         962     0:05:22        0.4809    14.67 GB    9.2 x 1e-5\n",
            "       180         962     0:05:41        0.4751    14.67 GB    9.1 x 1e-5\n",
            "       190         962     0:06:00        0.4663    14.67 GB    9.0 x 1e-5\n",
            "       200         962     0:06:19        0.4663    14.67 GB    8.8 x 1e-5\n",
            "\n",
            "Running Validation...\n",
            "  Batch     0  of    107.\n",
            "  Batch    40  of    107.\n",
            "  Batch    80  of    107.\n",
            "\n",
            "  Validation MCC: 0.65\n",
            "  Validation took: 0:01:39\n",
            "       210         962     0:08:16        0.4616    14.67 GB    8.7 x 1e-5\n",
            "       220         962     0:08:34        0.4656    14.67 GB    8.6 x 1e-5\n",
            "       230         962     0:08:53        0.4558    14.67 GB    8.5 x 1e-5\n",
            "       240         962     0:09:11        0.4598    14.67 GB    8.4 x 1e-5\n",
            "       250         962     0:09:29        0.4595    14.67 GB    8.3 x 1e-5\n",
            "       260         962     0:09:48        0.4545    14.67 GB    8.1 x 1e-5\n",
            "       270         962     0:10:06        0.4481    14.67 GB    8.0 x 1e-5\n",
            "       280         962     0:10:25        0.4532    14.67 GB    7.9 x 1e-5\n",
            "       290         962     0:10:43        0.4509    14.67 GB    7.8 x 1e-5\n",
            "       300         962     0:11:01        0.4537    14.67 GB    7.7 x 1e-5\n",
            "       310         962     0:11:20        0.4469    14.67 GB    7.6 x 1e-5\n",
            "       320         962     0:11:38        0.4571    14.67 GB    7.4 x 1e-5\n",
            "       330         962     0:11:57        0.4571    14.67 GB    7.3 x 1e-5\n",
            "       340         962     0:12:15        0.4579    14.67 GB    7.2 x 1e-5\n",
            "       350         962     0:12:33        0.4585    14.67 GB    7.1 x 1e-5\n",
            "       360         962     0:12:52        0.4555    14.67 GB    7.0 x 1e-5\n",
            "       370         962     0:13:10        0.4537    14.67 GB    6.9 x 1e-5\n",
            "       380         962     0:13:29        0.4482    14.67 GB    6.8 x 1e-5\n",
            "       390         962     0:13:47        0.4462    14.67 GB    6.6 x 1e-5\n",
            "       400         962     0:14:05        0.4425    14.67 GB    6.5 x 1e-5\n",
            "\n",
            "Running Validation...\n",
            "  Batch     0  of    107.\n",
            "  Batch    40  of    107.\n",
            "  Batch    80  of    107.\n",
            "\n",
            "  Validation MCC: 0.65\n",
            "  Validation took: 0:01:39\n",
            "       410         962     0:16:02        0.4401    14.67 GB    6.4 x 1e-5\n",
            "       420         962     0:16:21        0.4389    14.67 GB    6.3 x 1e-5\n",
            "       430         962     0:16:39        0.4347    14.67 GB    6.2 x 1e-5\n",
            "       440         962     0:16:58        0.4334    14.67 GB    6.1 x 1e-5\n",
            "       450         962     0:17:16        0.4316    14.67 GB    5.9 x 1e-5\n",
            "       460         962     0:17:34        0.4288    14.67 GB    5.8 x 1e-5\n",
            "       470         962     0:17:53        0.4259    14.67 GB    5.7 x 1e-5\n",
            "       480         962     0:18:11        0.4229    14.67 GB    5.6 x 1e-5\n",
            "       490         962     0:18:29        0.4227    14.67 GB    5.5 x 1e-5\n",
            "       500         962     0:18:48        0.4204    14.67 GB    5.4 x 1e-5\n",
            "       510         962     0:19:06        0.4198    14.67 GB    5.2 x 1e-5\n",
            "       520         962     0:19:25        0.4167    14.67 GB    5.1 x 1e-5\n",
            "       530         962     0:19:43        0.4156    14.67 GB    5.0 x 1e-5\n",
            "       540         962     0:20:01        0.4175    14.67 GB    4.9 x 1e-5\n",
            "       550         962     0:20:20        0.4195    14.67 GB    4.8 x 1e-5\n",
            "       560         962     0:20:38        0.4218    14.67 GB    4.7 x 1e-5\n",
            "       570         962     0:20:57        0.4194    14.67 GB    4.5 x 1e-5\n",
            "       580         962     0:21:15        0.4186    14.67 GB    4.4 x 1e-5\n",
            "       590         962     0:21:33        0.4152    14.67 GB    4.3 x 1e-5\n",
            "       600         962     0:21:52        0.4174    14.67 GB    4.2 x 1e-5\n",
            "\n",
            "Running Validation...\n",
            "  Batch     0  of    107.\n",
            "  Batch    40  of    107.\n",
            "  Batch    80  of    107.\n",
            "\n",
            "  Validation MCC: 0.66\n",
            "  Validation took: 0:01:39\n",
            "       610         962     0:23:49        0.4142    14.67 GB    4.1 x 1e-5\n",
            "       620         962     0:24:08        0.4146    14.67 GB    4.0 x 1e-5\n",
            "       630         962     0:24:26        0.4091    14.67 GB    3.9 x 1e-5\n",
            "       640         962     0:24:44        0.4069    14.67 GB    3.7 x 1e-5\n",
            "       650         962     0:25:03        0.4058    14.67 GB    3.6 x 1e-5\n",
            "       660         962     0:25:21        0.4045    14.67 GB    3.5 x 1e-5\n",
            "       670         962     0:25:39        0.4017    14.67 GB    3.4 x 1e-5\n",
            "       680         962     0:25:58        0.4013    14.67 GB    3.3 x 1e-5\n",
            "       690         962     0:26:16        0.4004    14.67 GB    3.2 x 1e-5\n",
            "       700         962     0:26:35        0.3992    14.67 GB    3.0 x 1e-5\n",
            "       710         962     0:26:53        0.3979    14.67 GB    2.9 x 1e-5\n",
            "       720         962     0:27:11        0.3968    14.67 GB    2.8 x 1e-5\n",
            "       730         962     0:27:30        0.3956    14.67 GB    2.7 x 1e-5\n",
            "       740         962     0:27:48        0.3958    14.67 GB    2.6 x 1e-5\n",
            "       750         962     0:28:06        0.3956    14.67 GB    2.5 x 1e-5\n",
            "       760         962     0:28:25        0.3947    14.67 GB    2.3 x 1e-5\n",
            "       770         962     0:28:43        0.3934    14.67 GB    2.2 x 1e-5\n",
            "       780         962     0:29:02        0.3914    14.67 GB    2.1 x 1e-5\n",
            "       790         962     0:29:20        0.3893    14.67 GB    2.0 x 1e-5\n",
            "       800         962     0:29:38        0.3888    14.67 GB    1.9 x 1e-5\n",
            "\n",
            "Running Validation...\n",
            "  Batch     0  of    107.\n",
            "  Batch    40  of    107.\n",
            "  Batch    80  of    107.\n",
            "\n",
            "  Validation MCC: 0.70\n",
            "  Validation took: 0:01:39\n",
            "       810         962     0:31:35        0.3876    14.67 GB    1.8 x 1e-5\n",
            "       820         962     0:31:54        0.3868    14.67 GB    1.6 x 1e-5\n",
            "       830         962     0:32:12        0.3870    14.67 GB    1.5 x 1e-5\n",
            "       840         962     0:32:31        0.3901    14.67 GB    1.4 x 1e-5\n",
            "       850         962     0:32:49        0.3888    14.67 GB    1.3 x 1e-5\n",
            "       860         962     0:33:07        0.3873    14.67 GB    1.2 x 1e-5\n",
            "       870         962     0:33:26        0.3873    14.67 GB    1.1 x 1e-5\n",
            "       880         962     0:33:44        0.3869    14.67 GB    1.0 x 1e-5\n",
            "       890         962     0:34:03        0.3864    14.67 GB    0.8 x 1e-5\n",
            "       900         962     0:34:21        0.3859    14.67 GB    0.7 x 1e-5\n",
            "       910         962     0:34:40        0.3854    14.67 GB    0.6 x 1e-5\n",
            "       920         962     0:34:58        0.3857    14.67 GB    0.5 x 1e-5\n",
            "       930         962     0:35:16        0.3839    14.67 GB    0.4 x 1e-5\n",
            "       940         962     0:35:35        0.3822    14.67 GB    0.3 x 1e-5\n",
            "       950         962     0:35:53        0.3816    14.67 GB    0.1 x 1e-5\n",
            "       960         962     0:36:11        0.3826    14.67 GB    0.0 x 1e-5\n",
            "\n",
            "Training complete!\n",
            "\n",
            "  Average training loss: 0.3824\n",
            "  Training took: 0:36:15\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Set the seed value again to make sure this is reproducible--the data\n",
        "# loader will select the same random batches every time we run this.\n",
        "seed_val = 42\n",
        "\n",
        "# Set it all over the place for good measure.\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss,\n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "t0 = time.time()\n",
        "\n",
        "total_train_loss = 0\n",
        "\n",
        "# Track the current step (because gradient accumulation means that this is\n",
        "# different than the number of forward passes on the GPU).\n",
        "optim_step = 0\n",
        "\n",
        "# Put the model into training mode. `dropout` and `batchnorm` layers behave\n",
        "# differently during training vs. test.\n",
        "model.train()\n",
        "\n",
        "# ========================================\n",
        "#               Training\n",
        "# ========================================\n",
        "\n",
        "# Perform one full pass over the training set.\n",
        "print('Training...\\n')\n",
        "\n",
        "print(\n",
        "\"'step' refers to one optimizer step (an important distinction if you're \\\n",
        "using gradient accumulation).\\n\"\n",
        ")\n",
        "\n",
        "# Print the header row for status updates.\n",
        "print(\"{:>10}  {:>10}  {:>10}  {:>12}  {:>10}  {:>12}\".format(\n",
        "    \"Step\", \"of Total\", \"Elapsed\", \"Loss\", \"GPU Mem\", \"lr\"))\n",
        "\n",
        "# For each GPU batch of training data...\n",
        "for gpu_batch_i, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # ===================\n",
        "    #    Forward Pass\n",
        "    # ===================\n",
        "\n",
        "    # Unpack this training batch from our dataloader.\n",
        "    #\n",
        "    # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "    # the `to` method.\n",
        "    #\n",
        "    # `batch` contains three pytorch tensors:\n",
        "    #   [0]: input ids\n",
        "    #   [1]: attention masks\n",
        "    #   [2]: labels -- In this case, these are target words at each\n",
        "    #                  position.\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    # Perform a forward pass (evaluate the model on this training batch).\n",
        "    # In PyTorch, calling `model` will in turn call the model's `forward`\n",
        "    # function and pass down the arguments.\n",
        "    # Specifically, we'll get the loss (because we provided labels) and the\n",
        "    # \"logits\"--the model outputs prior to activation.\n",
        "    result = model(\n",
        "        b_input_ids,\n",
        "        attention_mask = b_input_mask,\n",
        "        labels = b_labels\n",
        "    )\n",
        "\n",
        "    loss = result.loss\n",
        "    logits = result.logits\n",
        "\n",
        "    # Accumulate the training loss over all of the batches.\n",
        "    # Note that the loss is already averaged over the GPU batch size!\n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    # =======================\n",
        "    #     Backward Pass\n",
        "    # =======================\n",
        "\n",
        "    # Further scale the loss by the number of accumulation passes\n",
        "    loss = loss / accumulate_passes\n",
        "\n",
        "    # Perform a backward pass to calculate the gradients.\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the norm of the gradients to 1.0.\n",
        "    # This is to help prevent the \"exploding gradients\" problem.\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # Perform optimizer step after accumulating gradients.\n",
        "    if ((gpu_batch_i + 1) % accumulate_passes == 0) or \\\n",
        "       ((gpu_batch_i + 1) == len(train_dataloader)):\n",
        "\n",
        "        # Update the step count.\n",
        "        optim_step += 1\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate according to the schedule.\n",
        "        scheduler.step()\n",
        "\n",
        "        # Clear the gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ======================\n",
        "        #       Progress\n",
        "        # ======================\n",
        "\n",
        "        # Report every xx steps\n",
        "        if optim_step % 10 == 0:\n",
        "\n",
        "            # Calculate and format elapsed time.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Display the learning rate as a multiple of 1e-5.\n",
        "            lr_multiple = format_lr_as_multiple(scheduler.get_last_lr()[0])\n",
        "\n",
        "            # Print the current row with values.\n",
        "            print(\"{:>10,}  {:>10,}  {:>10}  {:>12.4f}  {:>10}  {:>12}\".format(\n",
        "                optim_step,\n",
        "                num_update_steps,\n",
        "                elapsed,\n",
        "                total_train_loss / optim_step,\n",
        "                gpu_mem_used(),\n",
        "                lr_multiple\n",
        "            ))\n",
        "\n",
        "        # ====================\n",
        "        #      Validation\n",
        "        # ====================\n",
        "        # Periodically measure our performance on our validation set.\n",
        "\n",
        "        if optim_step % 200 == 0:\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            val_t0 = time.time()\n",
        "\n",
        "            # Run the validation dataset through the model.\n",
        "            true_labels, predictions, avg_val_loss = evaluate(model, validation_dataloader)\n",
        "\n",
        "            # Gather metrics...\n",
        "            true_labels = np.asarray(true_labels)\n",
        "            predictions = np.asarray(predictions)\n",
        "\n",
        "            val_mcc = matthews_corrcoef(true_labels, predictions)\n",
        "\n",
        "            print(\"\\n  Validation MCC: {0:.2f}\".format(val_mcc))\n",
        "\n",
        "            # Measure how long the validation run took.\n",
        "            validation_time = format_time(time.time() - val_t0)\n",
        "\n",
        "            print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "            # Record statistics.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'Step': optim_step,\n",
        "                    'Training Loss': total_train_loss / (optim_step + 1),\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. MCC': val_mcc,\n",
        "                    'Training Time': format_time(time.time() - t0),\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "# =======================\n",
        "#   Training Complete\n",
        "# =======================\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"\")\n",
        "\n",
        "# Calculate the average loss over all of the batches.\n",
        "avg_train_loss = total_train_loss / num_update_steps\n",
        "\n",
        "print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
        "\n",
        "training_time = format_time(time.time() - t0)\n",
        "\n",
        "print(\"  Training took: {:}\".format(training_time))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQTvJ1vRP7u4"
      },
      "source": [
        "Let's view the summary of the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6O_NbXFGMukX",
        "outputId": "25284cbb-b658-4644-f2cd-ad4755050fe0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_stats\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Step\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 258,\n        \"min\": 200,\n        \"max\": 800,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          400,\n          800,\n          200\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Training Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03253874866721116,\n        \"min\": 0.38831224773845374,\n        \"max\": 0.46396659655306505,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.44144542429055045,\n          0.38831224773845374,\n          0.46396659655306505\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Valid. Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.027061417156610833,\n        \"min\": 0.3396650546656869,\n        \"max\": 0.4057165027082523,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.4057165027082523,\n          0.3396650546656869,\n          0.36871371073561293\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Valid. MCC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.024223998088256073,\n        \"min\": 0.6466732251295729,\n        \"max\": 0.7007057264811966,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6466732251295729,\n          0.7007057264811966,\n          0.6540128389857018\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Training Time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"0:15:44\",\n          \"0:31:17\",\n          \"0:07:58\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Validation Time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"0:01:39\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_stats"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-13995b16-1f5b-4918-8242-aaeaf63e72ee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. MCC</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>200</td>\n",
              "      <td>0.4640</td>\n",
              "      <td>0.3687</td>\n",
              "      <td>0.6540</td>\n",
              "      <td>0:07:58</td>\n",
              "      <td>0:01:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>400</td>\n",
              "      <td>0.4414</td>\n",
              "      <td>0.4057</td>\n",
              "      <td>0.6467</td>\n",
              "      <td>0:15:44</td>\n",
              "      <td>0:01:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>600</td>\n",
              "      <td>0.4167</td>\n",
              "      <td>0.3740</td>\n",
              "      <td>0.6596</td>\n",
              "      <td>0:23:31</td>\n",
              "      <td>0:01:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>800</td>\n",
              "      <td>0.3883</td>\n",
              "      <td>0.3397</td>\n",
              "      <td>0.7007</td>\n",
              "      <td>0:31:17</td>\n",
              "      <td>0:01:39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13995b16-1f5b-4918-8242-aaeaf63e72ee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13995b16-1f5b-4918-8242-aaeaf63e72ee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13995b16-1f5b-4918-8242-aaeaf63e72ee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5f15f327-d39f-4122-a5d6-8a5cb4f28ca6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f15f327-d39f-4122-a5d6-8a5cb4f28ca6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5f15f327-d39f-4122-a5d6-8a5cb4f28ca6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7759e38d-f7b0-488e-9232-fb39b34d5393\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_stats')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7759e38d-f7b0-488e-9232-fb39b34d5393 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_stats');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Step  Training Loss  Valid. Loss  Valid. MCC Training Time Validation Time\n",
              "0   200         0.4640       0.3687      0.6540       0:07:58         0:01:39\n",
              "1   400         0.4414       0.4057      0.6467       0:15:44         0:01:39\n",
              "2   600         0.4167       0.3740      0.6596       0:23:31         0:01:39\n",
              "3   800         0.3883       0.3397      0.7007       0:31:17         0:01:39"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('display.precision', 4)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-G03mmwH3aI"
      },
      "source": [
        "(For reference, we are using 7,695 training samples and 856 validation samples).\n",
        "\n",
        "Validation Loss is generally a more precise measure than accuracy, because with accuracy we don't care about the exact output value, but just which side of a threshold it falls on.\n",
        "\n",
        "If we are predicting the correct answer, but with less confidence, then validation loss will catch this, while accuracy will not.\n",
        "\n",
        "In this situation, though, the validation MCC is likely better because of the class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "68xreA9JAmG5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1Hx_WPMpRytA_GrnmYP1akcm38s8DsZcf' alt='Learning curve with training and validation loss' width='768' />\n"
      ],
      "metadata": {
        "id": "w5zFYfXlHpzr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "# S9. Performance On Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DosV94BYIYxg"
      },
      "source": [
        "Now we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using [Matthew's correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F"
      },
      "source": [
        "### 9.1. Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWe0_JW21MyV"
      },
      "source": [
        "\n",
        "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YuJF53dYagL"
      },
      "source": [
        "**Step 1: Load the data file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mAN0LZBOOPVh",
        "outputId": "35770558-3d52-43e5-e6fd-0c679a3af8fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ======== Load Test Set ========\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-XPpy7Lhmzs"
      },
      "source": [
        "Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n",
        "\n",
        "We use MCC here because the classes are imbalanced:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "idEWLUfnhmzt",
        "outputId": "dbdf08b0-1dc1-4755-c791-33af9117908c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ]
        }
      ],
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(),\n",
        "                                               len(df.label),\n",
        "                                            (df.label.sum() / len(df.label) * 100.0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSrGwRfFYgKg"
      },
      "source": [
        "**Step 2: Add prompt and labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MIPsE9agYYnK"
      },
      "outputs": [],
      "source": [
        "labeled_sentences = []\n",
        "labels_as_ids = []\n",
        "\n",
        "# For each sentence in the dataset...\n",
        "for i in range(len(sentences)):\n",
        "\n",
        "    sentence = sentences[i]\n",
        "    label_val = labels[i]\n",
        "\n",
        "    # Map the numerical label (0, 1) to the word we chose.\n",
        "    label_word = label_val_to_word[label_val]\n",
        "\n",
        "    # Look up the token id for the label.\n",
        "    label_id = label_val_to_token_id[label_val]\n",
        "\n",
        "    # Insert the sample and its label into the template.\n",
        "    labeled_sentence = prompt_template.format(\n",
        "        sentence = sentence,\n",
        "        label_word = label_word\n",
        "    )\n",
        "\n",
        "    # Add to our new lists.\n",
        "    labeled_sentences.append(labeled_sentence)\n",
        "    labels_as_ids.append(label_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CE-qZ_C4VGbf",
        "outputId": "66b4b27e-1faa-49c2-8280-4aecbb028544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's what they look like now:\n",
            "\n",
            "' Examples of sentences that are grammatically ' acceptable' or ' unacceptable':\n",
            " Him and me are going to the store. - unacceptable\n",
            " Him and I are going to the store. - acceptable\n",
            " Somebody just left - guess who. - acceptable'\n"
          ]
        }
      ],
      "source": [
        "print(\"Here's what they look like now:\\n\")\n",
        "print(\"'{:}'\".format(labeled_sentences[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVu4w5qgVVxx"
      },
      "source": [
        "**Step 4: Tokenize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dlT_Uk9pVEwO",
        "outputId": "3c899165-22c8-4029-f98b-1a139b8c1700"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ======== Tokenize ========\n",
        "\n",
        "# The tokenizer is a \"callable object\"--this invokes its __call__ function,\n",
        "# which will tokenize and encode all of the input strings.\n",
        "test_encodings = tokenizer(\n",
        "\n",
        "    labeled_sentences, # List of strings.\n",
        "\n",
        "    padding = 'longest',  # Pad out all of the samples to match the longest one\n",
        "                          # in the data.\n",
        "\n",
        "    #max_length = 64,      # An alternative strategy is to specify a maximum\n",
        "    #padding='max_length', # length, but it makes sense to let the tokenizer\n",
        "                           # figure that out.\n",
        "\n",
        "    truncation = True, # Truncate any samples longer than the model's maximum\n",
        "                       # sequence length.\n",
        "\n",
        "    add_special_tokens = True, # Add the bos and eos tokens.\n",
        "    return_token_type_ids = False, # These were used in BERT, but not in Mistral.\n",
        "    return_attention_mask = True, # Mistral uses attention masks.\n",
        "\n",
        "    return_tensors = \"pt\" # Return the results as pytorch tensors.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBeohhOGYwIF"
      },
      "source": [
        "**Step 5: Identify label positions and create masks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tuzIqSarVWDb"
      },
      "outputs": [],
      "source": [
        "# I'll add the prefix 'all' to these variables, since they still contain both\n",
        "# the training and validation data.\n",
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "test_target_words = []\n",
        "test_label_ids = []\n",
        "test_label_pos = []\n",
        "\n",
        "# For each of the encoded samples...\n",
        "for i in range(len(labels_as_ids)):\n",
        "\n",
        "    # Extract input_ids and attention_mask\n",
        "    input_ids = test_encodings['input_ids'][i]\n",
        "    attention_mask = test_encodings['attention_mask'][i]\n",
        "\n",
        "    # Find the position of the last non-padding token using the attention mask\n",
        "    # Because we appended the label to the end of the input, this is the\n",
        "    # position of our label word.\n",
        "    label_position = attention_mask.nonzero()[-1].item()\n",
        "\n",
        "    # This will tell the model what to token to predict at each position.\n",
        "    # (i.e., at position 12, the model should predict target_words[12])\n",
        "    # You can set the value to -100 for any tokens you don't want to train on,\n",
        "    # and in our case, we only want to train on the label.\n",
        "    # Start by filling it all out with -100s\n",
        "    target_words = torch.full_like(input_ids, -100)  # Initialize all labels to -100\n",
        "\n",
        "    # Get the token id for the label\n",
        "    label_id = labels_as_ids[i]\n",
        "\n",
        "    # We want all of the words / tokens masked out, except for the label.\n",
        "    target_words[label_position] = label_id\n",
        "\n",
        "    # Store everything.\n",
        "    test_input_ids.append(input_ids)\n",
        "    test_attention_masks.append(attention_mask)\n",
        "    test_target_words.append(target_words)\n",
        "    test_label_pos.append(label_position)\n",
        "    test_label_ids.append(label_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU4ODVAJgA9Z"
      },
      "source": [
        "Create our dataset and data loader for batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TuFbiCyXZGmh"
      },
      "outputs": [],
      "source": [
        "test_dataset = TensorDataset(\n",
        "    torch.stack(test_input_ids),\n",
        "    torch.stack(test_attention_masks),\n",
        "    torch.stack(test_target_words),\n",
        "    torch.tensor(test_label_ids),\n",
        "    torch.tensor(test_label_pos)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sQZpFZT74sgx"
      },
      "outputs": [],
      "source": [
        "# Set the batch size.\n",
        "test_batch_size = gpu_batch_size\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_sampler = SequentialSampler(test_dataset)\n",
        "\n",
        "prediction_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    sampler = prediction_sampler,\n",
        "    batch_size = test_batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lctEOyNFik"
      },
      "source": [
        "## 9.2. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhR99IISNMg9"
      },
      "source": [
        "\n",
        "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "948OA4qKZ37z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdae274f-ca2a-41ee-d6f5-1a55ae489dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "  Batch     0  of     65.\n",
            "  Batch    40  of     65.\n",
            "    DONE.\n"
          ]
        }
      ],
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(test_encodings['input_ids'])))\n",
        "\n",
        "true_labels, predictions, val_loss = evaluate(model, prediction_dataloader)\n",
        "\n",
        "print('    DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PLTzpNAAZ370",
        "outputId": "57f8769b-fc27-4c23-85d0-72bc1ed617eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flat accuracy: 85.27%\n",
            "\n",
            "Confusion Matrix:\n",
            "\n",
            "[[331  23]\n",
            " [ 53 109]]\n",
            "\n",
            "  TP    FP\n",
            "  FN    TN\n",
            "\n",
            "MCC: 0.647\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "true_labels = np.asarray(true_labels)\n",
        "predictions = np.asarray(predictions)\n",
        "\n",
        "# Report the final accuracy for this validation run.\n",
        "val_accuracy = float(np.sum(true_labels == predictions)) / len(true_labels) * 100.0\n",
        "print(\"Flat accuracy: {0:.2f}%\".format(val_accuracy))\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Print or log the confusion matrix\n",
        "print(\"\\nConfusion Matrix:\\n\")\n",
        "print(conf_matrix)\n",
        "print(\"\")\n",
        "print(\"  TP    FP\")\n",
        "print(\"  FN    TN\")\n",
        "\n",
        "mcc = matthews_corrcoef(true_labels, predictions)\n",
        "\n",
        "print(\"\\nMCC: {0:.3f}\".format(mcc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O5OAEkTzzQU"
      },
      "source": [
        "A score of MCC = `64.7` is impressive!\n",
        "\n",
        "For reference, my original [BERT example](https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=ex5O1eV-Pfct) scores `51.4`.\n",
        "\n",
        "The library documents the expected accuracy for this benchmark [here](https://huggingface.co/transformers/examples.html#glue) as `49.23`.\n",
        "\n",
        "You can also look at the official leaderboard [here](https://gluebenchmark.com/leaderboard/submission/zlssuBTm5XRs0aSKbFYGVIVdvbj1/-LhijX9VVmvJcvzKymxy). However, it's not a fair comparison--I've found that training on the _entire_ training set (i.e., not removing 10% for validatio as we did here) gives the accuracy a big boost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebyPfeMvBsIK"
      },
      "source": [
        "# ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ],
      "metadata": {
        "id": "VxbXxgPZYX0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n",
        "\n",
        "**Discord**\n",
        "\n",
        "Questions or Feedback? Leave a comment below, or better yet join us on our discord!\n",
        "\n",
        "[![Discord Button](https://lh3.googleusercontent.com/d/1kWYDt8JEJ-EXoaBWjZoil_d7W4bBQ9iy)](https://discord.gg/3QMCn7fNe5)\n",
        "\n",
        "--------\n"
      ],
      "metadata": {
        "id": "IMV-UpbRYaez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**\n",
        "\n",
        "* This [blog post](https://www.striveworks.com/blog/llms-for-text-classification-a-guide-to-supervised-learning) by Benjamin Nativi, at the company StriveWorks, was a great resource for understanding and comparing some of the possible approaches to applying an LLM to classification. In particular, it's how I learned about the technique of using the LM head for the classification step. The post doesn't include code, but their research was a helpful starting point.\n",
        "\n",
        "--------\n",
        "\n",
        "--------"
      ],
      "metadata": {
        "id": "TOJ8Q1enYkdE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyxHkLVCnK2-"
      },
      "source": [
        "# For funsies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnqPvry8mwdg",
        "outputId": "cd20b914-0c78-4368-f49f-3a3fcd34ffaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Llamas can output words. - Incorrect\n",
            " Llama 3.1 can output words. - Correct\n",
            " Llamas with version numbers output words. - Correct\n",
            " My pet llama, Tina, can talk. - Incorrect\n",
            " If I upgrade Tina to v3.1, she can talk. -\n",
            " Incorrect\n"
          ]
        }
      ],
      "source": [
        "# Let's see what you've got, Llama...\n",
        "logic_test = \"\"\"\n",
        " Llamas can output words. - Incorrect\n",
        " Llama 3.1 can output words. - Correct\n",
        " Llamas with version numbers output words. - Correct\n",
        " My pet llama, Tina, can talk. - Incorrect\n",
        " If I upgrade Tina to v3.1, she can talk. -\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(logic_test, return_tensors=\"pt\")\n",
        "\n",
        "# Generate the output from the model\n",
        "outputs = model.generate(**inputs.to(device), max_new_tokens=1)\n",
        "\n",
        "# Decode the generated tokens to text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the result\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8_3LvJLgZ3G"
      },
      "source": [
        "# GPU Memory Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndwv6Z-5Cxzc",
        "outputId": "3198da88-3008-4b57-c5c5-3733f9233d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory: 14.748 GB\n",
            "Compressed model: 5.85 GB\n",
            "Forward pass: 7.59 GB\n",
            "Adding LoRA: 7.67 GB\n",
            "Training: 14.68 GB\n"
          ]
        }
      ],
      "source": [
        "gpu_mem_train =  gpu_mem_used()\n",
        "\n",
        "gpu_memory = \"14.748 GB\"\n",
        "\n",
        "print(\"GPU Memory:\", gpu_memory)\n",
        "print(\"Compressed model:\", gpu_mem_model)\n",
        "print(\"Forward pass:\", gpu_mem_forward_pass)\n",
        "print(\"Adding LoRA:\", gpu_mem_lora)\n",
        "print(\"Training:\", gpu_mem_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yllqd_V0JLnB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Convert the string values to floats\n",
        "gm_model = float(gpu_mem_model.split()[0])\n",
        "gm_forward_pass = float(gpu_mem_forward_pass.split()[0])\n",
        "gm_lora = float(gpu_mem_lora.split()[0])\n",
        "gm_train = float(gpu_mem_train.split()[0])\n",
        "gm_total = float(gpu_memory.split()[0])\n",
        "\n",
        "# Calculate the incremental memory added by each step\n",
        "memory_additions = [\n",
        "    gm_model,  # Compressed Model\n",
        "    gm_forward_pass - gm_model,  # Forward Pass\n",
        "    gm_lora - gm_forward_pass,   # Adding LoRA\n",
        "    gm_train - gm_lora           # Training\n",
        "]\n",
        "\n",
        "# Update labels with memory added information\n",
        "labels = [\n",
        "    f'+{memory_additions[0]:.2f} GB - Compressed Model',\n",
        "    f'+{memory_additions[1]:.2f} GB - Forward Pass',\n",
        "    f'+{memory_additions[2]:.2f} GB - Adding LoRA',\n",
        "    f'+{memory_additions[3]:.2f} GB - Training'\n",
        "]\n",
        "\n",
        "# Create a stacked bar plot with updated labels and proper y-limit\n",
        "fig, ax = plt.subplots(figsize=(2, 5), dpi=150)  # Reduced figure width for a narrower bar, higher resolution\n",
        "\n",
        "# Plot a horizontal line representing total used.\n",
        "ax.axhline(y=gm_total, color='b', linestyle='--', label=f'Used: {gpu_mem_train}')\n",
        "\n",
        "# Plot a horizontal line representing the total GPU memory available.\n",
        "ax.axhline(y=gm_total, color='r', linestyle='--', label=f'{gpu:>4}: {gpu_memory}')\n",
        "\n",
        "# Use a stacked bar chart where each bar adds to the total\n",
        "ax.bar(\".\", memory_additions[0], label=labels[0], color='skyblue')\n",
        "ax.bar(\".\", memory_additions[1], bottom=memory_additions[0], label=labels[1], color='lightgreen')\n",
        "ax.bar(\".\", memory_additions[2], bottom=memory_additions[0] + memory_additions[1], label=labels[2], color='orange')\n",
        "ax.bar(\".\", memory_additions[3], bottom=memory_additions[0] + memory_additions[1] + memory_additions[2], label=labels[3], color='salmon')\n",
        "\n",
        "\n",
        "# Adjust the y-limit to make sure the total GPU memory line is visible\n",
        "ax.set_ylim(0, gm_total + 1)  # Add some space above the total GPU memory\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_ylabel('Memory Usage (GB)', fontsize=10)\n",
        "ax.set_title('GPU Batch: {:}\\n      Seq Len: {:}'.format(gpu_batch_size, len(encodings['input_ids'][0])), fontsize=10)\n",
        "\n",
        "# Reverse the legend order to match the order of the bars, and move it outside the plot\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "handles = list(reversed(handles))\n",
        "reversed_labels = list(reversed(labels))\n",
        "\n",
        "handles = handles[-2:] + handles[:-2]\n",
        "reversed_labels = reversed_labels[-2:] + reversed_labels[:-2]\n",
        "\n",
        "ax.legend(handles,\n",
        "          reversed_labels,\n",
        "          loc='center left', bbox_to_anchor=(1, 0.5),\n",
        "          prop={'size': 10, 'family': 'monospace'})\n",
        "\n",
        "# Adjust x-axis to make the single bar narrower visually\n",
        "ax.set_xlim(-0.75, 0.75)\n",
        "\n",
        "ax.tick_params(axis='y', labelsize=10)  # Adjust '8' to your desired font size\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1_clDHs44RuZqjGT_cKdvjlhf6C-SZJgo' alt=\"How the T4's memory was used\" width='512' />\n"
      ],
      "metadata": {
        "id": "5coFTAmNICpr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUmsUOIv8EUO"
      },
      "source": [
        "# Logging Runs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3717UYcInyc"
      },
      "source": [
        "### Summary of Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "ZvjDcKDF39Q0",
        "outputId": "9098f64d-2203-4051-f31a-3c5f1ed28e81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       Metric  \\\n",
              "0                   Timestamp   \n",
              "1                       Model   \n",
              "2                         GPU   \n",
              "3                  GPU Memory   \n",
              "4             GPU Memory Used   \n",
              "5                   Data Type   \n",
              "6            Memory for Model   \n",
              "7   Memory after Forward Pass   \n",
              "8    Memory after Adding LoRA   \n",
              "9       Memory after Training   \n",
              "10    Maximum Sequence Length   \n",
              "11       Effective Batch Size   \n",
              "12             GPU Batch Size   \n",
              "13     Gradient Checkpointing   \n",
              "14                  Optimizer   \n",
              "15          Raw Learning Rate   \n",
              "16              Learning Rate   \n",
              "17       Learning Rate x LoRA   \n",
              "18          Formatted Example   \n",
              "19                     LoRA r   \n",
              "20                 LoRA alpha   \n",
              "21               LoRA Dropout   \n",
              "22               Quantization   \n",
              "23              Training Loss   \n",
              "24              Training time   \n",
              "25                        MCC   \n",
              "26               Few-Shot MCC   \n",
              "\n",
              "                                                Value  \n",
              "0                               2024-10-23 - 14:38:10  \n",
              "1                                          LLaMA 3 8B  \n",
              "2                                                  T4  \n",
              "3                                           14.748 GB  \n",
              "4                                            14.68 GB  \n",
              "5                                       torch.float16  \n",
              "6                                             5.85 GB  \n",
              "7                                             7.59 GB  \n",
              "8                                             7.67 GB  \n",
              "9                                            14.68 GB  \n",
              "10                                                 86  \n",
              "11                                                  8  \n",
              "12                                                  8  \n",
              "13                                              False  \n",
              "14       <class 'bitsandbytes.optim.adamw.AdamW8bit'>  \n",
              "15                                             0.0001  \n",
              "16                                        10.0 x 1e-5  \n",
              "17                                        20.0 x 1e-5  \n",
              "18  '\" Examples of sentences that are grammaticall...  \n",
              "19                                                  8  \n",
              "20                                                 16  \n",
              "21                                               0.05  \n",
              "22                                              4-bit  \n",
              "23                                             0.3883  \n",
              "24                                            0:36:15  \n",
              "25                                             0.6466  \n",
              "26                                             0.5552  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c58df1be-f32a-4748-bca2-483f1330aba4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Timestamp</td>\n",
              "      <td>2024-10-23 - 14:38:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Model</td>\n",
              "      <td>LLaMA 3 8B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GPU</td>\n",
              "      <td>T4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GPU Memory</td>\n",
              "      <td>14.748 GB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GPU Memory Used</td>\n",
              "      <td>14.68 GB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Data Type</td>\n",
              "      <td>torch.float16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Memory for Model</td>\n",
              "      <td>5.85 GB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Memory after Forward Pass</td>\n",
              "      <td>7.59 GB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Memory after Adding LoRA</td>\n",
              "      <td>7.67 GB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Memory after Training</td>\n",
              "      <td>14.68 GB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Maximum Sequence Length</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Effective Batch Size</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>GPU Batch Size</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Gradient Checkpointing</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Optimizer</td>\n",
              "      <td>&lt;class 'bitsandbytes.optim.adamw.AdamW8bit'&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Raw Learning Rate</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Learning Rate</td>\n",
              "      <td>10.0 x 1e-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Learning Rate x LoRA</td>\n",
              "      <td>20.0 x 1e-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Formatted Example</td>\n",
              "      <td>'\" Examples of sentences that are grammaticall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>LoRA r</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>LoRA alpha</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>LoRA Dropout</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Quantization</td>\n",
              "      <td>4-bit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Training Loss</td>\n",
              "      <td>0.3883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Training time</td>\n",
              "      <td>0:36:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>MCC</td>\n",
              "      <td>0.6466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Few-Shot MCC</td>\n",
              "      <td>0.5552</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c58df1be-f32a-4748-bca2-483f1330aba4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c58df1be-f32a-4748-bca2-483f1330aba4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c58df1be-f32a-4748-bca2-483f1330aba4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4e08b693-f96f-4e52-aa2a-5a96bce22ebd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e08b693-f96f-4e52-aa2a-5a96bce22ebd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4e08b693-f96f-4e52-aa2a-5a96bce22ebd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_cc92536d-b72c-433f-baf7-00cc9ddfdcb5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_cc92536d-b72c-433f-baf7-00cc9ddfdcb5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('summary_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "summary_df",
              "summary": "{\n  \"name\": \"summary_df\",\n  \"rows\": 27,\n  \"fields\": [\n    {\n      \"column\": \"Metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 27,\n        \"samples\": [\n          \"Memory after Adding LoRA\",\n          \"Gradient Checkpointing\",\n          \"Memory after Training\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Value\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"7.67 GB\",\n          \"'\\\" Examples of sentences that are grammatically ' acceptable' or ' unacceptable':\\n Him and me are going to the store. - unacceptable\\n Him and I are going to the store. - acceptable\\n Somebody just left - guess who. - acceptable\\\"\",\n          \"2024-10-23 - 14:38:10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "summary = {}\n",
        "\n",
        "summary[\"Timestamp\"] = datetime.now().strftime(\"%Y-%m-%d - %H:%M:%S\")\n",
        "\n",
        "summary[\"Model\"] = \"LLaMA 3 8B\"\n",
        "\n",
        "# ==== System ====\n",
        "summary[\"GPU\"] = gpu\n",
        "summary[\"GPU Memory\"] = gpu_memory\n",
        "summary[\"GPU Memory Used\"] = gpu_mem_used()\n",
        "summary[\"Data Type\"] = str(torch_dtype)\n",
        "summary[\"Memory for Model\"] = gpu_mem_model\n",
        "summary[\"Memory after Forward Pass\"] = gpu_mem_forward_pass\n",
        "summary[\"Memory after Adding LoRA\"] = gpu_mem_lora\n",
        "summary[\"Memory after Training\"] = gpu_mem_train\n",
        "\n",
        "# ==== Training Parameters ====\n",
        "summary[\"Maximum Sequence Length\"] = len(encodings['input_ids'][0])\n",
        "summary[\"Effective Batch Size\"] = train_batch_size\n",
        "summary[\"GPU Batch Size\"] = gpu_batch_size\n",
        "#summary[\"Accumulate Batches\"] = accumulate_passes\n",
        "summary[\"Gradient Checkpointing\"] = False\n",
        "summary[\"Optimizer\"] = str(optimizer_class)\n",
        "summary[\"Raw Learning Rate\"] = lr\n",
        "summary[\"Learning Rate\"] = format_lr_as_multiple(lr)\n",
        "summary[\"Learning Rate x LoRA\"] = format_lr_as_multiple(\n",
        "    lr * lora_config.lora_alpha / lora_config.r\n",
        ")\n",
        "# Include the ' to tell sheets to treat this as a literal string and include the\n",
        "# surrounding quotes.\n",
        "summary[\"Formatted Example\"] = \"'\\\"{:}\\\"\".format(labeled_sentences[0])\n",
        "\n",
        "#summary[\"Weight Decay\"] = training_args.weight_decay\n",
        "#summary[\"Scheduler\"] = training_args.scheduler\n",
        "#summary[\"Steps\"] = training_args.max_steps\n",
        "\n",
        "# ==== QLoRA Parameters ====\n",
        "summary[\"LoRA r\"] = lora_config.r\n",
        "summary[\"LoRA alpha\"] = lora_config.lora_alpha\n",
        "summary[\"LoRA Dropout\"] = lora_config.lora_dropout\n",
        "#summary[\"LoRA Targets\"] = lora_config.target_modules\n",
        "summary[\"Quantization\"] = \"4-bit\"\n",
        "\n",
        "num_records = len(df_stats)\n",
        "\n",
        "# ==== Results ====\n",
        "summary[\"Training Loss\"] = df_stats['Training Loss'][num_records - 1]\n",
        "#summary[\"Valid. Loss\"] = df_stats['Valid. Loss'][epochs]\n",
        "summary[\"Training time\"] = training_time\n",
        "summary[\"MCC\"] = mcc\n",
        "summary[\"Few-Shot MCC\"] = few_shot_mcc\n",
        "\n",
        "\n",
        "# Convert the summary dictionary to a DataFrame\n",
        "summary_df = pd.DataFrame(list(summary.items()), columns=['Metric', 'Value'])\n",
        "\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjM6S4EQRYvy"
      },
      "source": [
        "### Summary Spreadsheet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W3Eh-L6IsTG"
      },
      "source": [
        "Install Required Libraries: Ensure you have the necessary libraries installed in your Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp6mA7ToIq2T",
        "outputId": "47aa952e-b31a-4383-c69c-79d599f14e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (6.0.2)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from gspread) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from gspread) (1.2.1)\n",
            "Requirement already satisfied: StrEnum==0.4.15 in /usr/local/lib/python3.10/dist-packages (from gspread) (0.4.15)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.12.0->gspread) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.12.0->gspread) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.12.0->gspread) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install gspread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18LhZkp1IrnD"
      },
      "source": [
        "Authenticate and Access Google Sheets: You need to authenticate and access your Google Sheets. Use the following code to authenticate your Google account and open the Google Sheet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhlobdbDumOL"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6bjAimbtw46"
      },
      "outputs": [],
      "source": [
        "# Open the Google Sheet by its id\n",
        "# This is 'Fine-Tuning StackLLaMA - Run Summaries'\n",
        "all_sheets = gc.open_by_key(\"1-EaX_HjYxZSU6BwJ1WT2eEC41nIYGeWoIjlYfD09_0s\")\n",
        "\n",
        "# Second sheet is for this CoLA notebook\n",
        "sheet = all_sheets.worksheets()[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNlUwV3O5lvd",
        "outputId": "2f71c3c5-057c-4036-b146-1a9225eea1e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Worksheet 'sheet1' id:0>,\n",
              " <Worksheet 'sheet2' id:1366635988>,\n",
              " <Worksheet 'Sheet3' id:1814563296>]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "all_sheets.worksheets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A8EzPHXI1Qr"
      },
      "source": [
        "Prepare the Data: Convert your summary dictionary into a format that can be appended to the Google Sheet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XJ3IMx6I34T"
      },
      "source": [
        "Append the Data to Google Sheets: Read the existing data, determine the next available column, and append the new summary data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXnLLMrxDtuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f37aa07-3b1e-4e6c-8492-6944c7939759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestamp     2024-10-23 - 14:38:10\n",
            "Model     LLaMA 3 8B\n",
            "GPU     T4\n",
            "GPU Memory     14.748 GB\n",
            "GPU Memory Used     14.68 GB\n",
            "Data Type     torch.float16\n",
            "Memory for Model     5.85 GB\n",
            "Memory after Forward Pass     7.59 GB\n",
            "Memory after Adding LoRA     7.67 GB\n",
            "Memory after Training     14.68 GB\n",
            "Maximum Sequence Length     86\n",
            "Effective Batch Size     8\n",
            "GPU Batch Size     8\n",
            "Gradient Checkpointing     False\n",
            "Optimizer     <class 'bitsandbytes.optim.adamw.AdamW8bit'>\n",
            "Raw Learning Rate     0.0001\n",
            "Learning Rate     10.0 x 1e-5\n",
            "Learning Rate x LoRA     20.0 x 1e-5\n",
            "Formatted Example     '\" Examples of sentences that are grammatically ' acceptable' or ' unacceptable':\n",
            " Him and me are going to the store. - unacceptable\n",
            " Him and I are going to the store. - acceptable\n",
            " Somebody just left - guess who. - acceptable\"\n",
            "LoRA r     8\n",
            "LoRA alpha     16\n",
            "LoRA Dropout     0.05\n",
            "Quantization     4-bit\n",
            "Training Loss     0.38831224773845374\n",
            "Training time     0:36:15\n",
            "MCC     0.6465701736731001\n",
            "Few-Shot MCC     0.5552395518145117\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the existing data in the sheet\n",
        "existing_data = sheet.get_all_values()\n",
        "existing_df = pd.DataFrame(existing_data)\n",
        "\n",
        "# Determine the next available column\n",
        "next_col = len(existing_df.columns) + 1\n",
        "\n",
        "# Append the summary data to the sheet\n",
        "for index, row in summary_df.iterrows():\n",
        "\n",
        "    # Find the 0-indexed row number for this metric from the summary table.\n",
        "    index_obj = existing_df[existing_df[0] == row['Metric']].index\n",
        "\n",
        "    # If the index object is empty, the metric wasn't found.\n",
        "    if len(index_obj) == 0:\n",
        "        print(\"Metric not found:\", row['Metric'])\n",
        "\n",
        "    else:\n",
        "        # The spreadsheet row number is 1-indexed.\n",
        "        sheet_row = index_obj[0] + 1\n",
        "\n",
        "        print(row['Metric'], '   ', row['Value'])\n",
        "\n",
        "        # Update the cell in the next available column\n",
        "        sheet.update_cell(sheet_row, next_col, row['Value'])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "002b3022f00447158bbac0cfc4d2e557": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_750b2d829cae4546b73c5cf520ccf3b6",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_517ef2f6710e443eb1c6c461b7ae27e5",
            "value": 4
          }
        },
        "0038bdd3a2604b23a449402acf7783e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_139b4baa7cc948428baaff4d05313f8a",
              "IPY_MODEL_e4c7785373574f5c90d301b90d27cfbd",
              "IPY_MODEL_768a7964725e41ca967e0c651fd2452a"
            ],
            "layout": "IPY_MODEL_eac4e680cf7a4b33ade0623896d27ed5"
          }
        },
        "026a1b99beb84eb987c20ab310725fb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "046b5e87f0a148fabcaa450bf35f7b30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cdcf8eedad443d090fa8d434a396b9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cf14fed4cfd49d798e3ce5adbe5ccc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12d1072820bf426cbd83243499e4ff59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "139b4baa7cc948428baaff4d05313f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd68eddd38514846816e622c2047b331",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_412bd5a883414296ac942d4ac71b593f",
            "value": "model-00002-of-00004.safetensors:‚Äá100%"
          }
        },
        "166671c5552c4fe69d3b520cd3c20ca6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17604738a4ac4141b0cb896b2a26a39c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b99c7bea81d46c489d3f9f285301a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_780353d8b2e04a718b679fe7a76400ab",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c9bbc091ab2b4b65acf72f022e10f048",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "1caabf687bc34fa0a1ecafe36ab6ce5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93f2063bb917485b87315a44d3ded4fe",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_97383e78650f484892dad6d6cabc35a8",
            "value": "model.safetensors.index.json:‚Äá100%"
          }
        },
        "1da251b5598541f48ff894d7a3dab9dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "237c97c0feea4ac1a1440e83525b83ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25e15a54c17f453480c5706f616d2de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2fa0ba2d7924e81b1d01f2f60eb4c8e",
              "IPY_MODEL_f08af515bc0b49f4b118f0110a38558b",
              "IPY_MODEL_cf6186da4e02447db484641ecb81d205"
            ],
            "layout": "IPY_MODEL_aa5d9e68044449869f81755e773db193"
          }
        },
        "2b4f8a36cb6b4c13b7b37dd526cff806": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ce8e917df9d4de8b05428a71f3e2c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1caabf687bc34fa0a1ecafe36ab6ce5a",
              "IPY_MODEL_3514fbccb2e24d8381222e57429c1905",
              "IPY_MODEL_b5c9d89d2b52480188440dfff333466c"
            ],
            "layout": "IPY_MODEL_56fc2f63070c422198186b8fbc305e19"
          }
        },
        "2fde1e66d5124620a876866ee1e14e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67630f80bcaf48b6b192c2e210629ad0",
            "max": 1168138808,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45634f5ca52f44d99ee66f0e9bcf50a8",
            "value": 1168138808
          }
        },
        "3081fbac79e94351a6ae95c5de07380e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3507f0de88ab4580a2f64426d2ad2dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3514fbccb2e24d8381222e57429c1905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b95ec09868a44c52a2b15f9c87f5319d",
            "max": 23950,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be607af3d75e48ac90958122edf84e1b",
            "value": 23950
          }
        },
        "374a18e9c2014581bc59d45c937c0353": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba1de3629793438088412ccd79d47111",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6cbdaf2ee2b34f5aa7fd874af22fb76f",
            "value": "‚Äá1.17G/1.17G‚Äá[00:27&lt;00:00,‚Äá42.4MB/s]"
          }
        },
        "3b226e31455c4192bd6dd5c7ea860680": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b6192d160d64c349aca0a756606fdd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e32853ab3e044aeb6613f9937c258fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ccb1ebf2d24f485ab91188e4a1276e18",
              "IPY_MODEL_ac6386392c7e43fdaafd2e9127fa0931",
              "IPY_MODEL_61a449db0e224784ba89e7492466665d"
            ],
            "layout": "IPY_MODEL_59543d512c2b4ce4a308723b41b22945"
          }
        },
        "412bd5a883414296ac942d4ac71b593f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41d673eddc2246ec88421d3adfd32e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "450f78d39afb407d83d169099980300e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45634f5ca52f44d99ee66f0e9bcf50a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "485ff9a93d274b4fb1321a75dfcd0197": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cdcf8eedad443d090fa8d434a396b9f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e059c97af9af4c8d8841b1aca85ff2eb",
            "value": "model-00004-of-00004.safetensors:‚Äá100%"
          }
        },
        "4a89d6829b4b4f1184637f24423c8a5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "517ef2f6710e443eb1c6c461b7ae27e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56fc2f63070c422198186b8fbc305e19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5720212272c2430f8de878f92a1d7767": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59543d512c2b4ce4a308723b41b22945": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "599c2ccb9ad542b5afc9e87a830e3229": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59a75224171e4b40bd031d536165a979": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bb6b22c9020493a8996261e0e2f3e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c272b96f8ff04302978a7375696d2585",
              "IPY_MODEL_dfebcbc64f3a4db1bf858d79a6301240",
              "IPY_MODEL_ba682f0940e6490985806850564f9324"
            ],
            "layout": "IPY_MODEL_c83d36786f0d44699da2cc226e366f6d"
          }
        },
        "6069ab63d3f2426c910cc126ac1a1a78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a449db0e224784ba89e7492466665d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffcb164625bf4013be19412fd44c4596",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0cf14fed4cfd49d798e3ce5adbe5ccc5",
            "value": "‚Äá4.92G/4.92G‚Äá[01:56&lt;00:00,‚Äá42.0MB/s]"
          }
        },
        "6442742f0797426f9a4c0966ba5efd44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "661cb899b91a4c1998b8d8c8d864d66d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67630f80bcaf48b6b192c2e210629ad0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6837604b81fe4646bdaed7e3572ae75e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "689dfac524bf4a08af176576c93f1c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cbb4fe6700f4e39a27e402455ce2f84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cbdaf2ee2b34f5aa7fd874af22fb76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d36f5da28a3464aac48765abba44154": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3ad9a56cc74b48903c6e06c9af90d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7f0262a77e942008248717a8dc38e72",
              "IPY_MODEL_e3e17d5392b84a529d4d8640d5edfd6d",
              "IPY_MODEL_dcbfbc7f137a423d8a99e40daa23077e"
            ],
            "layout": "IPY_MODEL_6442742f0797426f9a4c0966ba5efd44"
          }
        },
        "7397fead1a02487fba4e0345524199ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17604738a4ac4141b0cb896b2a26a39c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_85e16aba176c4bbf8122a4535b8cbeec",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "750b2d829cae4546b73c5cf520ccf3b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "768a7964725e41ca967e0c651fd2452a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6069ab63d3f2426c910cc126ac1a1a78",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ed791d9f94ee45b2baa9c581ac1d3393",
            "value": "‚Äá5.00G/5.00G‚Äá[01:58&lt;00:00,‚Äá42.0MB/s]"
          }
        },
        "780353d8b2e04a718b679fe7a76400ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "789d14ded536464f918d4bdb9bb0d511": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d18766e76f544368d60a48c8779f4c8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3081fbac79e94351a6ae95c5de07380e",
            "value": "‚Äá177/177‚Äá[00:00&lt;00:00,‚Äá14.2kB/s]"
          }
        },
        "7994b237adf34ba4a17bb09c12e82293": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efc588c836c3467789107f07ac439a63",
              "IPY_MODEL_a89c9223186f4cb9bf61a5397b5fb757",
              "IPY_MODEL_ce5211faba2b4fe5be1be2234ee69510"
            ],
            "layout": "IPY_MODEL_599c2ccb9ad542b5afc9e87a830e3229"
          }
        },
        "7ba2fb21ed06496f85692202845971ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe856fecb6b04b9cb2b31ca28560eee8",
            "max": 73,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_026a1b99beb84eb987c20ab310725fb8",
            "value": 73
          }
        },
        "7d18766e76f544368d60a48c8779f4c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7edafa5c7d614068a9e095b8be9cd164": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_485ff9a93d274b4fb1321a75dfcd0197",
              "IPY_MODEL_2fde1e66d5124620a876866ee1e14e38",
              "IPY_MODEL_374a18e9c2014581bc59d45c937c0353"
            ],
            "layout": "IPY_MODEL_12d1072820bf426cbd83243499e4ff59"
          }
        },
        "811fdb60c51e4284b5d81246aba2a2e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82397fac6ae548439c88be7a51113b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "850d39ba957843a198344d2aabe8da5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b6192d160d64c349aca0a756606fdd4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_be5b0fc5b18945e584bd34031ae8ceb8",
            "value": "‚Äá4/4‚Äá[00:10&lt;00:00,‚Äá‚Äá2.30s/it]"
          }
        },
        "85e16aba176c4bbf8122a4535b8cbeec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "867ab6eef96240ff829aa43170eca2be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "886a4cb2659d4dbab3a92e4bd5d0d39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b99c7bea81d46c489d3f9f285301a1b",
              "IPY_MODEL_7ba2fb21ed06496f85692202845971ab",
              "IPY_MODEL_cffc2fcc78fe455aa742d58f958e360d"
            ],
            "layout": "IPY_MODEL_f17dacb729f9425ab63be302d0a1e2f1"
          }
        },
        "8ac10cb452f24e69be750244c9c418fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f76ce0b8f96043358ce49926638d82d8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3507f0de88ab4580a2f64426d2ad2dbe",
            "value": "‚Äá9.09M/9.09M‚Äá[00:00&lt;00:00,‚Äá20.1MB/s]"
          }
        },
        "8dd415f2c45046b7b5db22963e8b212c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8eaf622ece844de2a00ce689941750f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9190880502544e2fa42953da4951602e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92eca86514d94b6faf8d0d8ca68af879": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92f495a5e2d84cc4853e406e47372e5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93f2063bb917485b87315a44d3ded4fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "941c7294db704fd8806c38b5f53122d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97383e78650f484892dad6d6cabc35a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b4fba2e53934d6ab250b3f7ecf868a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_450f78d39afb407d83d169099980300e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1da251b5598541f48ff894d7a3dab9dd",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "9c8eb4b960a04acbbce2265849b19c56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d2d613e4d244c578e7f1faae1eb04be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e96009e0d1e4067af69bb97eda37296": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2fa0ba2d7924e81b1d01f2f60eb4c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abac6daa19454bf3ac10a3bb0f6ca80d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8eaf622ece844de2a00ce689941750f0",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "a661efbb8fd14bc3b5bf04269f7fbe8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a89c9223186f4cb9bf61a5397b5fb757": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cbb4fe6700f4e39a27e402455ce2f84",
            "max": 654,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_941c7294db704fd8806c38b5f53122d2",
            "value": 654
          }
        },
        "aa5d9e68044449869f81755e773db193": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aac2e57098094fa8a47fdb5fc4106f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7397fead1a02487fba4e0345524199ea",
              "IPY_MODEL_bc5cc9a5d26e4937a3f40d36540db378",
              "IPY_MODEL_789d14ded536464f918d4bdb9bb0d511"
            ],
            "layout": "IPY_MODEL_6837604b81fe4646bdaed7e3572ae75e"
          }
        },
        "abac6daa19454bf3ac10a3bb0f6ca80d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac6386392c7e43fdaafd2e9127fa0931": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0ace65b65d94bfca7487d35eeb3943f",
            "max": 4915916176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dd415f2c45046b7b5db22963e8b212c",
            "value": 4915916176
          }
        },
        "ad4e8c85524d49b28145f577c47fd224": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af2a47006c5b4d2cbd4d9504301019d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0ace65b65d94bfca7487d35eeb3943f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b53df4bf66c042f68f6c8d8fcd847603": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa893bdb0afe44899a1c9f17d21ce8b2",
              "IPY_MODEL_ecbbe66244e3436db171eef9f8a1ab9a",
              "IPY_MODEL_8ac10cb452f24e69be750244c9c418fd"
            ],
            "layout": "IPY_MODEL_edf9aa84db0f4f31b26681738929f928"
          }
        },
        "b5c9d89d2b52480188440dfff333466c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c565d6e63b354de3864fd9ecd032362c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_92eca86514d94b6faf8d0d8ca68af879",
            "value": "‚Äá23.9k/23.9k‚Äá[00:00&lt;00:00,‚Äá1.59MB/s]"
          }
        },
        "b95ec09868a44c52a2b15f9c87f5319d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba1de3629793438088412ccd79d47111": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba682f0940e6490985806850564f9324": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_237c97c0feea4ac1a1440e83525b83ca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_db21f4c419f14fe69ba6326228e953c8",
            "value": "‚Äá4/4‚Äá[06:21&lt;00:00,‚Äá82.19s/it]"
          }
        },
        "ba6c0f99cccc47048c7af6f4ef9d1aab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc5cc9a5d26e4937a3f40d36540db378": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d36f5da28a3464aac48765abba44154",
            "max": 177,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dbe1894e43f449e3852883200d4ee8a3",
            "value": 177
          }
        },
        "be5b0fc5b18945e584bd34031ae8ceb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be607af3d75e48ac90958122edf84e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c272b96f8ff04302978a7375696d2585": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceb4e27721d9400ab5139316fe4bc50d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9d2d613e4d244c578e7f1faae1eb04be",
            "value": "Downloading‚Äáshards:‚Äá100%"
          }
        },
        "c5131e49275447d1bc30007e5ffc9ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c565d6e63b354de3864fd9ecd032362c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c83d36786f0d44699da2cc226e366f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9bbc091ab2b4b65acf72f022e10f048": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccb1ebf2d24f485ab91188e4a1276e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_046b5e87f0a148fabcaa450bf35f7b30",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_82397fac6ae548439c88be7a51113b07",
            "value": "model-00003-of-00004.safetensors:‚Äá100%"
          }
        },
        "cd68eddd38514846816e622c2047b331": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce5211faba2b4fe5be1be2234ee69510": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a89d6829b4b4f1184637f24423c8a5f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_811fdb60c51e4284b5d81246aba2a2e1",
            "value": "‚Äá654/654‚Äá[00:00&lt;00:00,‚Äá53.6kB/s]"
          }
        },
        "ceb4e27721d9400ab5139316fe4bc50d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf6186da4e02447db484641ecb81d205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a661efbb8fd14bc3b5bf04269f7fbe8a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ec402c957ee5443b9e4ecec3e65b06be",
            "value": "‚Äá50.6k/50.6k‚Äá[00:00&lt;00:00,‚Äá883kB/s]"
          }
        },
        "cffc2fcc78fe455aa742d58f958e360d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e24939e3a5ce42a6831ee3a3366874eb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dc81102677e84a4f86bae105104e3f7f",
            "value": "‚Äá73.0/73.0‚Äá[00:00&lt;00:00,‚Äá5.18kB/s]"
          }
        },
        "db21f4c419f14fe69ba6326228e953c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbe1894e43f449e3852883200d4ee8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc81102677e84a4f86bae105104e3f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcbfbc7f137a423d8a99e40daa23077e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_661cb899b91a4c1998b8d8c8d864d66d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9e96009e0d1e4067af69bb97eda37296",
            "value": "‚Äá4.98G/4.98G‚Äá[01:58&lt;00:00,‚Äá42.5MB/s]"
          }
        },
        "dfebcbc64f3a4db1bf858d79a6301240": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_166671c5552c4fe69d3b520cd3c20ca6",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b226e31455c4192bd6dd5c7ea860680",
            "value": 4
          }
        },
        "e059c97af9af4c8d8841b1aca85ff2eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e24939e3a5ce42a6831ee3a3366874eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3e17d5392b84a529d4d8640d5edfd6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d673eddc2246ec88421d3adfd32e7b",
            "max": 4976698672,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_867ab6eef96240ff829aa43170eca2be",
            "value": 4976698672
          }
        },
        "e4c7785373574f5c90d301b90d27cfbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b4f8a36cb6b4c13b7b37dd526cff806",
            "max": 4999802720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_689dfac524bf4a08af176576c93f1c5b",
            "value": 4999802720
          }
        },
        "e7f0262a77e942008248717a8dc38e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af2a47006c5b4d2cbd4d9504301019d6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_edac01c6d4434ab08baa976d3f9d1acd",
            "value": "model-00001-of-00004.safetensors:‚Äá100%"
          }
        },
        "eac4e680cf7a4b33ade0623896d27ed5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec402c957ee5443b9e4ecec3e65b06be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecbbe66244e3436db171eef9f8a1ab9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92f495a5e2d84cc4853e406e47372e5f",
            "max": 9085698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5720212272c2430f8de878f92a1d7767",
            "value": 9085698
          }
        },
        "ed791d9f94ee45b2baa9c581ac1d3393": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edac01c6d4434ab08baa976d3f9d1acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edf9aa84db0f4f31b26681738929f928": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efc588c836c3467789107f07ac439a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba6c0f99cccc47048c7af6f4ef9d1aab",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c5131e49275447d1bc30007e5ffc9ea5",
            "value": "config.json:‚Äá100%"
          }
        },
        "f08af515bc0b49f4b118f0110a38558b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9190880502544e2fa42953da4951602e",
            "max": 50566,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad4e8c85524d49b28145f577c47fd224",
            "value": 50566
          }
        },
        "f17dacb729f9425ab63be302d0a1e2f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d8b2141fee43548ebec527896cfcc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f76ce0b8f96043358ce49926638d82d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f92a47e287a44eac8a5ef74a1f057ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b4fba2e53934d6ab250b3f7ecf868a8",
              "IPY_MODEL_002b3022f00447158bbac0cfc4d2e557",
              "IPY_MODEL_850d39ba957843a198344d2aabe8da5a"
            ],
            "layout": "IPY_MODEL_9c8eb4b960a04acbbce2265849b19c56"
          }
        },
        "fa893bdb0afe44899a1c9f17d21ce8b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59a75224171e4b40bd031d536165a979",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f2d8b2141fee43548ebec527896cfcc9",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "fe856fecb6b04b9cb2b31ca28560eee8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffcb164625bf4013be19412fd44c4596": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}